@inproceedings{Carterette2009,
author = {Carterette, Ben and Chandar, Praveen},
title = {Probabilistic Models of Ranking Novel Documents for Faceted Topic Retrieval},
booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
series = {CIKM '09},
year = {2009},
isbn = {978-1-60558-512-3},
location = {Hong Kong, China},
pages = {1287--1296},
numpages = {10},
url = {http://doi.acm.org/10.1145/1645953.1646116},
doi = {10.1145/1645953.1646116},
acmid = {1646116},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {diversity, information retrieval, novelty, probabilistic models},
abstract = {Traditional models of information retrieval assume documents are independently relevant. But when the goal is retrieving diverse or novel information about a topic, retrieval models need to capture dependencies between documents. Such tasks require alternative evaluation and optimization methods that operate on different types of relevance judgments. We define faceted topic retrieval as a particular novelty-driven task with the goal of finding a set of documents that cover the different facets of an information need. A faceted topic retrieval system must be able to cover as many facets as possible with the smallest number of documents. We introduce two novel models for faceted topic retrieval, one based on pruning a set of retrieved documents and one based on retrieving sets of documents through direct optimization of evaluation measures. We compare the performance of our models to MMR and the probabilistic model due to Zhai et al. on a set of 60 topics annotated with facets, showing that our models are competitive.}
}

@inproceedings{Chandar2009,
author={Chandar, Praveen and Kailasam, Aparna and Muppaneni, Divya and Thota, Sree Lekha and Carterette, Ben},
title = {{Ad hoc and diversity retrieval at the University of Delaware}},
booktitle = {Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA},
series = {TREC '09},
year = {2009},
pages = {1--8},
abstract = {This is the report on the University of Delaware Information Retrieval Lab's participation in the TREC 2009 Web and Million Query tracks. Our report on the Relevance Feedback track is in a separate document titled Minimal test collections for relevance feedback.}
}

@inproceedings{Carterette2009b,
author={Carterette, Ben and Chandar, Praveen and Kailasam, Aparna and Muppaneni, Divya and Thota, Lekha},
title = {Minimal Test Collections for Relevance Feedback},
booktitle = {Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA},
series = {TREC '09},
year = {2009},
pages = {1--6},
abstract = {The Information Retrieval Lab at the University of Delaware participated in the Relevance Feedback track at TREC 2009. We used only the Category B subset of the ClueWeb collection; our preprocessing and indexing steps are described in our paper on ad hoc and diversity runs. The second year of the Relevance Feedback track focused on selection of documents for feedback. Our hypothesis is that documents that are good at distinguishing systems in terms of their effectiveness by mean average precision will also be good documents for relevance feedback. Thus we have applied the document selection algorithm MTC (Minimal Test Collections) developed by Carterette et al.  that is used in the Million Query Track for selecting documents to be judged to find the right ranking of systems. Our approach can therefore be described as "MTC for Relevance Feedback".}
}


@inproceedings{Chandar2010,
author = {Chandar, Praveen and Carterette, Ben},
title = {Diversification of Search Results Using Webgraphs},
booktitle = {Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
series = {SIGIR '10},
year = {2010},
isbn = {978-1-4503-0153-4},
location = {Geneva, Switzerland},
pages = {869--870},
numpages = {2},
url = {http://doi.acm.org/10.1145/1835449.1835657},
doi = {10.1145/1835449.1835657},
acmid = {1835657},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {diversity, information retrieval, webgraphs},
abstract = {A set of words is often insufficient to express a user's information need. In order to account for various information needs associated with a query, diversification seems to be a reasonable strategy. By diversifying the result set, we increase the probability of results being relevant to the user's information needs when the given query is ambiguous. A diverse result set must contain a set of documents that cover various subtopics for a given query. We propose a graph based method which exploits the link structure of the web to return a ranked list that provides complete coverage for a query. Our method not only provides diversity to the results set, but also avoids excessive redundancy. Moreover, the probability of relevance of a document is conditioned on the documents that appear before it in the result list. We show the effectiveness of our method by comparing it with a query-likelihood model as the baseline.}
}


@inproceedings{Carterette2010,
author={Carterette, Ben and Chandar, Praveen},
title = {Sessions, Diversity, and Ad Hoc Retrieval},
booktitle = {Proceedings of The Ninth Text REtrieval Conference, TREC 2010, Gaithersburg, Maryland, USA},
series = {TREC '10},
year = {2010},
pages = {1--6},
abstract = {}
}

@inproceedings{Chandar2011,
author = {Chandar, Praveen and Carterette, Ben},
title = {Analysis of Various Evaluation Measures for Diversity},
journal = {Proceedings of the 1st International Workshop on Diversity in Document Retrieval at European Conference on Information Retrieval},
year = {2011},
series = {ECIR '11},
pages = {21--28},
abstract = {Evaluation measures play a vital role in analyzing the performance of a system, comparing two or more systems, and optimizing systems to perform some task. In this paper, we analyze and highlight the strengths and weaknesses of commonly used measures for evaluating the diversity in search results. We compare MAP-IA, alpha-nDCG, and ERR-IA using data from TREC'09 web track diversity runs and simulated data. We describe a class of test sets that could be used to compare evaluation measure and systems used for diversifying search results.}
}

@inproceedings{Carterette2011b,
author = {Carterette, Ben and Chandar, Praveen},
title = {Implicit Feedback and Document Filtering for Retrieval Over Query Sessions},
booktitle = {Proceedings of The Twentieth Text REtrieval Conference, TREC 2011, Gaithersburg, Maryland, USA},
series = {TREC '11},
year = {2011},
pages = {1--3},
abstract = {}
}

@inproceedings{Chandar2012,
author = {Chandar, Praveen and Carterette, Ben},
title = {Using PageRank to Infer User Preferences},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
series = {SIGIR '12},
year = {2012},
isbn = {978-1-4503-1472-5},
location = {Portland, Oregon, USA},
pages = {1167--1168},
numpages = {2},
url = {http://doi.acm.org/10.1145/2348283.2348522},
doi = {10.1145/2348283.2348522},
acmid = {2348522},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {PageRank, preference judgments},
abstract = {Recently, researchers have shown interest in the use of preference judgments for evaluation in IR literature. Although preference judgments have several advantages over absolute judgment, one of the major disadvantages is that the number of judgments needed increases polynomially as the number of documents in the pool increases. We propose a novel method using PageRank to minimize the number of judgments required to evaluate systems using preference judgments. We test the proposed hypotheses using the TREC 2004 to 2006 Terabyte dataset to show that it is possible to reduce the evaluation cost considerably. Further, we study the susceptibility of the methods due to assessor errors.}
}

@inproceedings{Chandar2012b,
author = {Chandar, Praveen and Carterette, Ben},
title = {What Qualities Do Users Prefer in Diversity Rankings?},
booktitle = {Proceedings of the 2nd International Workshop on Diversity in Document Retrieval at Web Search and Data Mining Conference},
year = {2012},
series = {WSDM '11},
pages = {10--16},
abstract = {}
}

@inproceedings{Webber2012,
author = {Webber, William and Chandar, Praveen and Carterette, Ben},
title = {Alternative Assessor Disagreement and Retrieval Depth},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
series = {CIKM '12},
year = {2012},
isbn = {978-1-4503-1156-4},
location = {Maui, Hawaii, USA},
pages = {125--134},
numpages = {10},
url = {http://doi.acm.org/10.1145/2396761.2396781},
doi = {10.1145/2396761.2396781},
acmid = {2396781},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {evaluation, retrieval experiment, sampling},
abstract = {Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. In this paper, we examine the relationship between the rank at which a document is returned by a set of retrieval systems and the likelihood that assessors will disagree on its relevance, and find that there is a strong and consistent correlation between the two. We adopt a meta-rank method of summarizing a document's rank across multiple runs, and propose a logistic regression predictive model of assessor disagreement given meta-rank and initially-assessed relevance. The consistency of the model parameters across different topics, assessor pairs, and collections is considered. The model gives comparatively accurate predictions of absolute scores, but less consistent predictions of relative scores than a simpler rank-insensitive model. We demonstrate that the logistic regression model is robust to using sampled, rather than exhaustive, dual assessment. We demonstrate the use of the sampled predictive model to incorporate assessor disagreement into tests of statistical significance.}
}

@inproceedings{Chandar2012a,
author = {Chandar, Praveen and Carterette, Ben},
title = {Using Preference Judgments for Novel Document Retrieval},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
series = {SIGIR '12},
year = {2012},
isbn = {978-1-4503-1472-5},
location = {Portland, Oregon, USA},
pages = {861--870},
numpages = {10},
url = {http://doi.acm.org/10.1145/2348283.2348398},
doi = {10.1145/2348283.2348398},
acmid = {2348398},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {diversity, preference judgments, user study},
abstract = {There has been considerable interest in incorporating diversity in search results to account for redundancy and the space of possible user needs. Most work on this problem is based on subtopics: diversity rankers score documents against a set of hypothesized subtopics, and diversity rankings are evaluated by assigning a value to each ranked document based on the number of novel (and redundant) subtopics it is relevant to. This can be seen as modeling a user who is always interested in seeing more novel subtopics, with progressively decreasing interest in seeing the same subtopic multiple times. We put this model to test: if it is correct, then users, when given a choice, should prefer to see a document that has more value to the evaluation. We formulate some specific hypotheses from this model and test them with actual users in a novel preference-based design in which users express a preference for document A or document B given document C. We argue that while the user study shows the subtopic model is good, there are many other factors apart from novelty and redundancy that may be influencing user preferences. From this, we introduce a new framework to construct an ideal diversity ranking using only preference judgments, with no explicit subtopic judgments whatsoever.}
}

@inproceedings{Chandar2013a,
author = {Chandar, Praveen and Carterette, Ben},
 title = {Preference Based Evaluation Measures for Novelty and Diversity},
 booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '13},
 year = {2013},
 isbn = {978-1-4503-2034-4},
 location = {Dublin, Ireland},
 pages = {413--422},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2484028.2484094},
 doi = {10.1145/2484028.2484094},
 acmid = {2484094},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {evaluation, novelty and diversity},
 abstract = {Novel and diverse document ranking is an effective strategy that involves reducing redundancy in a ranked list to maximize the amount of novel and relevant information available to users. Evaluation for novelty and diversity typically involves an assessor judging each document for relevance against a set of pre-identified subtopics, which may be disambiguations of the query, facets of an information need, or nuggets of information. Alternately, when expressing a preference for document A or document B, users may implicitly take subtopics into account, but may also take into account other factors such as recency, readability, length, and so on, each of which may have more or less importance depending on user. A user profile contains information about the extent to which each factor, including subtopic relevance, plays a role in the user's preference for one document over another. A preference-based evaluation can then take this user profile information into account to better model utility to the space of users.
In this work, we propose an evaluation framework that not only can consider implicit factors but also handles differences in user preference due to varying underlying information need. Our proposed framework is based on the idea that a user scanning a ranked list from top to bottom and stopping at rank k gains some utility from every document that is relevant their information need. Thus, we model the expected utility of a ranked list by estimating the utility of a document at a given rank using preference judgments and define evaluation measures based on the same. We validate our framework by comparing it to existing measures such as alpha-nDCG, ERR-IA, and subtopic recall that require explicit subtopic judgments We show that our proposed measures correlate well with existing measures while having the potential to capture various other factors when real data is used. We also show that the proposed measures can easily handle relevance assessments against multiple user profiles, and that they are robust to noisy and incomplete judgments.
}
}


@inproceedings{Chandar2013,
author = {Chandar, Praveen and Webber, William and Carterette, Ben},
title = {Document Features Predicting Assessor Disagreement},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
series = {SIGIR '13},
year = {2013},
isbn = {978-1-4503-2034-4},
location = {Dublin, Ireland},
pages = {745--748},
numpages = {4},
url = {http://doi.acm.org/10.1145/2484028.2484161},
doi = {10.1145/2484028.2484161},
acmid = {2484161},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {evaluation, retrieval experiment},
abstract = {The notion of relevance differs between assessors, thus giving rise to assessor disagreement. Although assessor disagreement has been frequently observed, the factors leading to disagreement are still an open problem. In this paper we study the relationship between assessor disagreement and various topic independent factors such as readability and cohesiveness. We build a logistic model using reading level and other simple document features to predict assessor disagreement and rank documents by decreasing probability of disagreement. We compare the predictive power of these document-level features with that of a meta-search feature that aggregates a document's ranking across multiple retrieval runs. Our features are shown to be on a par with the meta-search feature, without requiring a large and diverse set of retrieval runs to calcu- late. Surprisingly, however, we find that the reading level features are negatively correlated with disagreement, suggesting that they are detecting some other aspect of document content.
}
}

@inproceedings{Bah2014,
author = {Bah, A and Carterette, B and Chandar, P},
title = {Udel @ NTCIR-11 IMine Track},
booktitle = {Proceedings of the 11th NTCIR Conference},
series = {NTCIR '11},
year = {2014},
pages = {80--83},
abstract = {This paper describes our participation in the Intent Mining track
of NTCIR-11. We present our methods and results for both
document ranking and subtopic mining. Our ranking methods are
based on several data fusion techniques with some variations. Our
subtopic mining method is a very simple technique that uses query
dimensions' items to form a subtopic.}
}


@phdthesis{ChandarThesis2014,
  author = {Chandar, Praveen},
  month = {August},
  school = {University of Delaware},
  title = {Novelty and Diversity in Search Results},
  year = {2014},
  series = {Phd. Thesis},
  pages = {1--175},
  abstract = {Information retrieval (IR) is the process of obtaining relevant information for a given information need. The concept of relevance and its relation to information needs is of central concern to IR researchers. Until recently, much work in IR settled with a notion of relevance that is topical -- that is, containing information about a specified topic -- and in which the relevance of a document in a ranking is independent of the relevance of other documents in the ranking. But such an approach is more likely to produce a ranking with a high degree of redundancy; the amount of novel information available to the user may be minimal as they traverse down a ranked list.


In this work, we focus on the novelty and diversity problem that models relevance of a document taking into account the inter-document effects in a ranked list and diverse information needs for a given query. Existing approaches to this problem mostly rely on identifying subtopics (disambiguation, facets, or other component parts) of an information need, then estimating a document's relevance independently w.r.t each subtopic. Users are treated as being satisfied by a ranking of documents that covers the space of subtopics as well as covering each individual subtopic sufficiently.

We propose a novel approach that models novelty implicitly while retaining the ability to capture other important factors affecting user satisfaction.
We formulate a set of hypotheses based on the existing subtopic approach and test them with actual users using a simple conditional preference design: users express a preference for document A or document B given document C.
Following this, we introduce a novel triplet framework for collecting such preference judgments and using them to estimate the total utility of a document while taking inter-document effects into account.
Finally, a set of utility-based metrics are proposed and validated to measure the effectiveness of a system for the novelty and diversity task. }
}


@inproceedings{Bah2015,
author = {Bah, Ashraf and Chandar, Praveen and Carterette, Ben},
title = {Document Comprehensiveness and User Preferences in Novelty Search Tasks},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
series = {SIGIR '15},
year = {2015},
isbn = {978-1-4503-3621-5},
location = {Santiago, Chile},
pages = {735--738},
numpages = {4},
url = {http://doi.acm.org/10.1145/2766462.2767820},
doi = {10.1145/2766462.2767820},
acmid = {2767820},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {diversity, preference judgment, user study},
abstract = {Different users may be attempting to satisfy different information needs while providing the same query to a search engine. Addressing that issue is addressing Novelty and Diversity in information retrieval. Novelty and Diversity search task models the task wherein users are interested in seeing more and more documents that are not only relevant, but also cover more aspects (or subtopics) related to the topic of interest. This is in contrast with the traditional IR task where topical relevance is the only factor in evaluating search results. In this paper, we conduct a user study where users are asked to give a preference between one of two documents B and C given a query and also given that they have already seen a document A. We then test a total of ten hypotheses pertaining to the relationship between the "comprehensiveness" of documents (i.e. the number of subtopics a document is relevant to) and real users' preference judgments. Our results show that users are inclined to prefer documents with higher comprehensiveness, even when the prior document A already covers more aspects than the two documents being compared, and even when the least preferred has a higher relevance grade. In fact, users are inclined to prefer documents with higher overall aspect-coverage even in cases where B and C are relevant to the same number of novel subtopics.}
}

@inproceedings{Chandar2015,
author = {Chandar, Praveen and Yaman, Anil and Hoxha, Julia and He, Zhe and Weng, Chunhua },
title = {Similarity-based Recommendation of New Concepts to a Terminology},
series = {AMIA '15},
booktitle = {Proceedings of AMIA 2015 Annual Symposium},
year = {2015},
pages = {386--395},
abstract = {Terminologies can suffer from poor concept coverage due to delays in new concept insertion. This study tests a similarity-based approach to recommending concepts from a text corpus to a terminology. Our approach involves extraction of candidate concepts from a given text corpus, which are represented using a set of features. The model learns the important features to characterize a concept and recommends new concepts to a terminology. Further, we propose a cost-effect evaluation methodology to estimate the effectiveness of terminology enrichment methods. To test our methodology, we use the clinical trial eligibility criteria free-text as an example text corpus to recommend concepts for SNOMED CT. We computed precision at various rank intervals to measure the performance of the methods. Results indicate that our automated algorithm is an effective method for concept recommendation.}
}

@inproceedings{He2015_draft,
author = {He, Zhe and Chandar, Praveen and Ryan, Patrick and Weng, Chunhua },
title = {{Simulation-based Evaluation of the Generalizability Index for Study Traits}},
series = {AMIA '15},
booktitle = {Proceedings of AMIA 2015 Annual Symposium},
year = {2015},
pages = {593--602},
abstract = {The Generalizability Index for Study Traits (GIST) has been proposed recently for assessing the population representativeness of a set of related clinical trials using eligibility features (e.g., age or BMI), one each time. However, GIST has not yet been evaluated. To bridge this knowledge gap, this paper reports a simulation-based validation study for GIST. Using the National Health and Nutrition Examination Survey (NHANES) data, we demonstrated the effectiveness of GIST at quantifying the population representativeness of a set of related trials that differ in disease domains, study phases, sponsor types, and study designs, respectively. We also showed that among seven example medical conditions, the GIST of age increases from Phase I trials to Phase III trials in the seven disease domains and is the lowest in asthma trials. We concluded that GIST correlates with simulation-based generalizability results and is a valid metric for quantifying population representativeness of related clinical trials.}
}

@inproceedings{Hruby2015,
author = {Hruby, Greg and Chandar, Praveen and Hoxha, Julia and Mandonca, Eneida and Hanauer, David and Weng, Chunhua },
title = {What Are Frequent Data Requests from Researchers? A Conceptual Model of Researchers' EHR Data Needs for Comparative Effectiveness Research Podium abstract},
series = {AMIA '15},
booktitle = {Proceedings of AMIA 2015 Annual Symposium},
year = {2015},
pages = {Podium Abstract},
abstract = {Data request forms are the key communication media linking medical researchers and informaticians. (1) The Carpenter framework serves as a representative mental model for how researchers conceptually organize information used for research. (2) Additionally, this model complements the Patient, Intervention, Control/Comparison, and Outcome nodes of the PICO framework, which is used for medical information retrieval.3-5 We hypothesized that the semantic structural similarities between the two models suggest the Carpenter model may be well-suited as a standard template for data needs specification.  As such, we choose the Carpenter model as a foundation for seeking a conceptual model to represent and organize common data needs of biomedical researchers. }
}

@article{Hoxha2016,
author = {Hoxha, Julia and Chandar, Praveen and He, Zhe and Cimino, James and Hanauer, David and Weng, Chunhua},
title = {{DREAM: Classification scheme for dialog acts in clinical research query mediation}},
journal = {Journal of Biomedical Informatics},
year = {2016},
volume = {59},
issue = {1},
pages = {89--101},
abstract = {Clinical data access involves complex, but opaque communication between medical researchers and query analysts. Understanding such communication is indispensable for designing intelligent human-machine dialog systems that automate query formulation. This study investigates email communication and proposes a novel scheme for classifying dialog acts in clinical research query mediation. We analyzed 315 email messages exchanged in the communication for 20 data requests obtained from three institutions. The messages were segmented into 1333 utterance units. Through a rigorous process, we developed a classification scheme and applied it for dialog act annotation of the extracted utterances. Evaluation results with high inter-annotator agreement demonstrate the reliability of this scheme. This dataset is used to contribute preliminary understanding of dialog acts distribution and conversation flow in this dialog space.}
}
