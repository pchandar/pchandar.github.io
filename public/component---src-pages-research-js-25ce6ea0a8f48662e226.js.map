{"version":3,"sources":["webpack:///component---src-pages-research-js-25ce6ea0a8f48662e226.js","webpack:///./src/pages/research.js"],"names":["webpackJsonp","158","module","exports","__webpack_require__","_interopRequireDefault","obj","__esModule","default","_react","_react2","_gatsbyLink","ResearchPage","createElement","className"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,EAASC,GAEhC,YAYA,SAASC,GAAuBC,GAAO,MAAOA,IAAOA,EAAIC,WAAaD,GAAQE,QAASF,GAVvFH,EAAQI,YAAa,CCPtB,IAAAE,GAAAL,EAAA,GDWKM,EAAUL,EAAuBI,GCVtCE,EAAAP,EAAA,IAEMQ,GDYcP,EAAuBM,GCZtB,iBACnBD,GAAAF,QAAAK,cAAA,OAAKC,UAAU,QACbJ,EAAAF,QAAAK,cAAA,+BAEAH,EAAAF,QAAAK,cAAA,sEAEAH,EAAAF,QAAAK,cAAA,8kCAGAH,EAAAF,QAAAK,cAAA,uEACAH,EAAAF,QAAAK,cAAA,imBAEAH,EAAAF,QAAAK,cAAA,mDACAH,EAAAF,QAAAK,cAAA,+tBD6CHV,GAAQK,QCxCMI,EDyCdV,EAAOC,QAAUA,EAAiB","file":"component---src-pages-research-js-25ce6ea0a8f48662e226.js","sourcesContent":["webpackJsonp([41579278739242],{\n\n/***/ 158:\n/***/ (function(module, exports, __webpack_require__) {\n\n\t'use strict';\n\t\n\texports.__esModule = true;\n\t\n\tvar _react = __webpack_require__(1);\n\t\n\tvar _react2 = _interopRequireDefault(_react);\n\t\n\tvar _gatsbyLink = __webpack_require__(36);\n\t\n\tvar _gatsbyLink2 = _interopRequireDefault(_gatsbyLink);\n\t\n\tfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\t\n\tvar ResearchPage = function ResearchPage() {\n\t  return _react2.default.createElement(\n\t    'div',\n\t    { className: 'text' },\n\t    _react2.default.createElement(\n\t      'h1',\n\t      null,\n\t      'Research Projects'\n\t    ),\n\t    _react2.default.createElement(\n\t      'h3',\n\t      null,\n\t      'Models and Measures for Novel And Diverse Search results'\n\t    ),\n\t    _react2.default.createElement(\n\t      'p',\n\t      null,\n\t      ' Traditional models of information retrieval assume documents are independently relevant. But modeling documents as independently relevant is more likely to produce a ranking with a high degree of redundancy; the amount of novel information available to the user may be minimal as they traverse down a ranked list. This research project attempts to remedy this with new models of document interdependence and new evaluation measures. There are three threads running through this work: (1) the models of diversity, novelty, and redundancy that will be needed to implement ranking algorithms; (2) measurements of diversity, novelty, and redundancy in a ranking of documents; and (3) optimizing model structures and parameters to the measures. As part of this project, we developed statistical models to reduce the degree of redundancy in a rank list, analyzed various evaluation frameworks, designed and conducted user studies to better understand user\\u2019s need and proposed a novel preference based evaluation framework. This project resulted in various conference publications and a thesis. '\n\t    ),\n\t    _react2.default.createElement(\n\t      'h3',\n\t      null,\n\t      'Predicting Assessor Disagreement in Information Retrieval'\n\t    ),\n\t    _react2.default.createElement(\n\t      'p',\n\t      null,\n\t      'Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. As part of this project, we studied the relationship between assessor disagreement and various factors such as readability, cohesiveness, and rank at which a document is returned by a set of retrieval systems (meta-rank). To this end, we proposed a logistic regression predictive model of second assessor disagreement given meta-rank and initially-assessed relevance. This project is resulted in various conference publications.'\n\t    ),\n\t    _react2.default.createElement(\n\t      'h3',\n\t      null,\n\t      'Evaluation of Answer Snippet Clusters'\n\t    ),\n\t    _react2.default.createElement(\n\t      'p',\n\t      null,\n\t      'Intelligence analysts are often confronted with informal sources of text including blogs and forums, and would like to study relationships among people involved in the discussion, and points of view expressed regarding a specific event. The problem involves a deep analysis of the large amounts of text. A typical system build for this problem returns a list of relevant answer snippets for a given natural language question and it is often a good idea to cluster (group) the returned answer snippets based on some criteria. In this project, our focus was on developing a novel approach to evaluate the snippet cluster returned by the system. This project resulted in a novel user interface to annotate snippet clusters.'\n\t    )\n\t  );\n\t};\n\t\n\texports.default = ResearchPage;\n\tmodule.exports = exports['default'];\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// component---src-pages-research-js-25ce6ea0a8f48662e226.js","import React from 'react'\nimport Link from 'gatsby-link'\n\nconst ResearchPage = () => (\n  <div className=\"text\">\n    <h1>Research Projects</h1>\n\n    <h3>Models and Measures for Novel And Diverse Search results</h3>\n\n    <p> Traditional models of information retrieval assume documents are independently relevant. But modeling documents as independently relevant is more likely to produce a ranking with a high degree of redundancy; the amount of novel information available to the user may be minimal as they traverse down a ranked list. This research project attempts to remedy this with new models of document interdependence and new evaluation measures. There are three threads running through this work: (1) the models of diversity, novelty, and redundancy that will be needed to implement ranking algorithms; (2) measurements of diversity, novelty, and redundancy in a ranking of documents; and (3) optimizing model structures and parameters to the measures. As part of this project, we developed statistical models to reduce the degree of redundancy in a rank list, analyzed various evaluation frameworks, designed and conducted user studies to better understand userâ€™s need and proposed a novel preference based evaluation framework. This project resulted in various conference publications and a thesis. </p>\n\n\n    <h3>Predicting Assessor Disagreement in Information Retrieval</h3>\n    <p>Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. As part of this project, we studied the relationship between assessor disagreement and various factors such as readability, cohesiveness, and rank at which a document is returned by a set of retrieval systems (meta-rank). To this end, we proposed a logistic regression predictive model of second assessor disagreement given meta-rank and initially-assessed relevance. This project is resulted in various conference publications.</p>\n\n    <h3>Evaluation of Answer Snippet Clusters</h3>\n    <p>Intelligence analysts are often confronted with informal sources of text including blogs and forums, and would like to study relationships among people involved in the discussion, and points of view expressed regarding a specific event. The problem involves a deep analysis of the large amounts of text. A typical system build for this problem returns a list of relevant answer snippets for a given natural language question and it is often a good idea to cluster (group) the returned answer snippets based on some criteria. In this project, our focus was on developing a novel approach to evaluate the snippet cluster returned by the system. This project resulted in a novel user interface to annotate snippet clusters.</p>\n\n  </div>\n)\n\nexport default ResearchPage\n\n\n\n// WEBPACK FOOTER //\n// ./src/pages/research.js"],"sourceRoot":""}