{"version":3,"sources":["webpack:///path---publication-da588abbcfe7c0b829b2.js","webpack:///./.cache/json/publication.json"],"names":["webpackJsonp","276","module","exports","data","allPubMetaJson","edges","node","title","details","authors","year","month","ptype","pdf","bib","abstract","allFile","name","publicURL","pathContext"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,gBAAkBC,QAAUC,MAAQC,MAAA,qHAAAC,QAAA,gIAAAC,SAAA,qCAAAC,KAAA,OAAAC,MAAA,KAAAC,MAAA,aAAAC,IAAA,eAAAC,IAAA,GAAAC,SAAA,45BAA4yCT,MAAQC,MAAA,sFAAAC,QAAA,kIAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,KAAAC,MAAA,aAAAC,IAAA,cAAAC,IAAA,cAAAC,SAAA,wnCAA8+CT,MAAQC,MAAA,8BAAAC,QAAA,uEAAAC,SAAA,uCAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,aAAAC,IAAA,cAAAC,IAAA,cAAAC,SAAA,6WAAunBT,MAAQC,MAAA,0EAAAC,QAAA,4IAAAC,SAAA,uCAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,cAAAC,IAAA,cAAAC,SAAA,41CAA0tDT,MAAQC,MAAA,8EAAAC,QAAA,iHAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,gBAAAC,IAAA,qBAAAC,IAAA,qBAAAC,SAAA,6gCAAy3CT,MAAQC,MAAA,kDAAAC,QAAA,4HAAAC,SAAA,sEAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,aAAAC,IAAA,sBAAAC,IAAA,sBAAAC,SAAA,g2BAAouCT,MAAQC,MAAA,6EAAAC,QAAA,2HAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,aAAAC,IAAA,sBAAAC,IAAA,sBAAAC,SAAA,MAA0XT,MAAQC,MAAA,4CAAAC,QAAA,uHAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,aAAAC,IAAA,qBAAAC,IAAA,qBAAAC,SAAA,MAAmVT,MAAQC,MAAA,sFAAAC,QAAA,8IAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,qBAAAC,IAAA,qBAAAC,SAAA,wnCAAwgDT,MAAQC,MAAA,+DAAAC,QAAA,4HAAAC,SAAA,sEAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,aAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,4QAAmpBT,MAAQC,MAAA,oDAAAC,QAAA,4IAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,q6BAA2wCT,MAAQC,MAAA,wDAAAC,QAAA,4JAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,WAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,miBAAw5BT,MAAQC,MAAA,2CAAAC,QAAA,8IAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,urBAAshCT,MAAQC,MAAA,0DAAAC,QAAA,4IAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,mBAAAC,IAAA,mBAAAC,SAAA,+wCAA6nDT,MAAQC,MAAA,wDAAAC,QAAA,wJAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,WAAAC,IAAA,mBAAAC,IAAA,mBAAAC,SAAA,MAAyXT,MAAQC,MAAA,qDAAAC,QAAA,4IAAAC,SAAA,0CAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,ghCAAm4CT,MAAQC,MAAA,iEAAAC,QAAA,4IAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,mBAAAC,IAAA,mBAAAC,SAAA,qiEAA05ET,MAAQC,MAAA,mEAAAC,QAAA,0EAAAC,SAAA,sDAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,gBAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,m4BAA+sCT,MAAQC,MAAA,2EAAAC,QAAA,4EAAAC,SAAA,+FAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,qtCAAmlDT,MAAQC,MAAA,oDAAAC,QAAA,8HAAAC,SAAA,8BAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,gBAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,82CAAysDT,MAAQC,MAAA,+EAAAC,QAAA,4IAAAC,SAAA,yEAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,kBAAAC,IAAA,kBAAAC,SAAA,2nCAAuiDT,MAAQC,MAAA,0CAAAC,QAAA,mDAAAC,SAAA,cAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,YAAAC,IAAA,wBAAAC,IAAA,wBAAAC,SAAA,m9DAA8sET,MAAQC,MAAA,+EAAAC,QAAA,sHAAAC,SAAA,mGAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,iBAAAC,IAAA,iBAAAC,SAAA,8xGAA4sHT,MAAQC,MAAA;AAAAC,QAAA,0EAAAC,SAAA,0CAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,gBAAAC,IAAA,mBAAAC,IAAA,mBAAAC,SAAA,4/BAAw0CT,MAAQC,MAAA,oFAAAC,QAAA,qFAAAC,SAAA,oEAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,UAAAC,IAAA,gBAAAC,IAAA,gBAAAC,SAAA,o6BAA+wCT,MAAQC,MAAA,6FAAAC,QAAA,2DAAAC,SAAA,6EAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,aAAAC,IAAA,aAAAC,SAAA,+kCAAk7CT,MAAQC,MAAA,qFAAAC,QAAA,2GAAAC,SAAA,6HAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,eAAAC,IAAA,eAAAC,SAAA,48BAA24CT,MAAQC,MAAA,wDAAAC,QAAA,6HAAAC,SAAA,0CAAAC,KAAA,OAAAC,MAAA,GAAAC,MAAA,gBAAAC,IAAA,iBAAAC,IAAA,iBAAAC,SAAA,kpCAAw/CT,MAAQC,MAAA,qHAAAC,QAAA,8IAAAC,SAAA,qCAAAC,KAAA,OAAAC,MAAA,EAAAC,MAAA,gBAAAC,IAAA,gBAAAC,IAAA,gBAAAC,SAAA,65BAAw0CC,SAAYX,QAAUC,MAAQW,KAAA,cAAAC,UAAA,8DAA+FZ,MAAQW,KAAA,cAAAC,UAAA,8DAA+FZ,MAAQW,KAAA,cAAAC,UAAA,8DAA+FZ,MAAQW,KAAA,cAAAC,UAAA,8DAA+FZ,MAAQW,KAAA,cAAAC,UAAA,8DAA+FZ,MAAQW,KAAA,kBAAAC,UAAA,kEAAuGZ,MAAQW,KAAA,KAAAC,UAAA,qDAA6EZ,MAAQW,KAAA,iBAAAC,UAAA,iEAAqGZ,MAAQW,KAAA,YAAAC,UAAA,4DAA2FZ,MAAQW,KAAA,eAAAC,UAAA,+DAAiGZ,MAAQW,KAAA,eAAAC,UAAA,+DAAiGZ,MAAQW,KAAA,aAAAC,UAAA,6DAA6FZ,MAAQW,KAAA,YAAAC,UAAA,4DAA2FZ,MAAQW,KAAA,WAAAC,UAAA,2DAAyFZ,MAAQW,KAAA,cAAAC,UAAA,8DAA+FZ,MAAQW,KAAA,iBAAAC,UAAA,iEAAqGZ,MAAQW,KAAA,aAAAC,UAAA,6DAA6FZ,MAAQW,KAAA,YAAAC,UAAA,4DAA2FZ,MAAQW,KAAA,cAAAC,UAAA,8DAA+FZ,MAAQW,KAAA,cAAAC,UAAA,8DAA+FZ,MAAQW,KAAA,eAAAC,UAAA,+DAAiGZ,MAAQW,KAAA,eAAAC,UAAA,+DAAiGZ,MAAQW,KAAA,SAAAC,UAAA,yDAAqFZ,MAAQW,KAAA,oBAAAC,UAAA,sEAA4GC","file":"path---publication-da588abbcfe7c0b829b2.js","sourcesContent":["webpackJsonp([141926880579864],{\n\n/***/ 276:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"allPubMetaJson\":{\"edges\":[{\"node\":{\"title\":\"An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings\",\"details\":\"Proceedings of the 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8-12 2018\",\"authors\":[\"D. Seyler\",\"P. Chandar\",\"M. Davis\"],\"year\":\"2018\",\"month\":null,\"ptype\":\"conference\",\"pdf\":\"sigir18b.pdf\",\"bib\":\"\",\"abstract\":\"We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion. Our method represents users, documents and other context-related documents as heterogeneous objects in a HIN. Using meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space. This allows inferences of user interest on unseen objects based on distance in the embedding space. These object distances are then incorporated as features in a well-established learning to rank (LTR) framework. We make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS task.\"}},{\"node\":{\"title\":\"Offline Comparative Evaluation with Incremental, Minimally-Invasive Online Feedback\",\"details\":\"Proceedings of the 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8-12, 2018.\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2018\",\"month\":null,\"ptype\":\"conference\",\"pdf\":\"sigir18.pdf\",\"bib\":\"sigir18.bib\",\"abstract\":\"We investigate the use of logged user interaction data---queries and clicks---for offline evaluation of new search systems in the context of counterfactual analysis.  The challenge of evaluating a new ranker against log data collected from a static production ranker is that new rankers may retrieve documents that have never been seen in the logs before, and thus lack any logged feedback from users.  Additionally, the ranker itself could bias user actions such that even  documents that have been seen in the logs would have exhibited different interaction patterns had they been retrieved and ranked by the new ranker.  We present a methodology for incrementally logging interactions on previously-unseen documents for use in computation of an unbiased estimator of a new ranker's effectiveness.  Our method is very lightly invasive with respect to the production ranker results to insure against users becoming dissatisfied if the new ranker is poor. We demonstrate how well our methods work in a simulation environment designed to be challenging for such methods to argue that they are likely to work in a wide variety of scenarios.\"}},{\"node\":{\"title\":\"Udel @ NTCIR-11 IMine Track\",\"details\":\"Proceedings of the 11th NTCIR Conference, December 2014, Pages 80-83\",\"authors\":[\"A. Bah\",\"B. Carterette\",\"P. Chandar\"],\"year\":\"2014\",\"month\":12,\"ptype\":\"TECHREPORT\",\"pdf\":\"bah2014.pdf\",\"bib\":\"bah2014.bib\",\"abstract\":\"This paper describes our participation in the Intent Mining track\\nof NTCIR-11. We present our methods and results for both\\ndocument ranking and subtopic mining. Our ranking methods are\\nbased on several data fusion techniques with some variations. Our\\nsubtopic mining method is a very simple technique that uses query\\ndimensions' items to form a subtopic.\"}},{\"node\":{\"title\":\"Document Comprehensiveness and User Preferences in Novelty Search Tasks\",\"details\":\"Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2015, Pages 735-738\",\"authors\":[\"A. Bah\",\"P. Chandar\",\"B. Carterette\"],\"year\":\"2015\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"bah2015.pdf\",\"bib\":\"bah2015.bib\",\"abstract\":\"Different users may be attempting to satisfy different information needs while providing the same query to a search engine. Addressing that issue is addressing Novelty and Diversity in information retrieval. Novelty and Diversity search task models the task wherein users are interested in seeing more and more documents that are not only relevant, but also cover more aspects (or subtopics) related to the topic of interest. This is in contrast with the traditional IR task where topical relevance is the only factor in evaluating search results. In this paper, we conduct a user study where users are asked to give a preference between one of two documents B and C given a query and also given that they have already seen a document A. We then test a total of ten hypotheses pertaining to the relationship between the \\\"comprehensiveness\\\" of documents (i.e. the number of subtopics a document is relevant to) and real users' preference judgments. Our results show that users are inclined to prefer documents with higher comprehensiveness, even when the prior document A already covers more aspects than the two documents being compared, and even when the least preferred has a higher relevance grade. In fact, users are inclined to prefer documents with higher overall aspect-coverage even in cases where B and C are relevant to the same number of novel subtopics.\"}},{\"node\":{\"title\":\"Probabilistic Models of Ranking Novel Documents for Faceted Topic Retrieval\",\"details\":\"Proceedings of the 18th ACM Conference on Information and Knowledge Management, November 2009, Pages 1287-1296\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2009\",\"month\":11,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"carterette2009.pdf\",\"bib\":\"carterette2009.bib\",\"abstract\":\"Traditional models of information retrieval assume documents are independently relevant. But when the goal is retrieving diverse or novel information about a topic, retrieval models need to capture dependencies between documents. Such tasks require alternative evaluation and optimization methods that operate on different types of relevance judgments. We define faceted topic retrieval as a particular novelty-driven task with the goal of finding a set of documents that cover the different facets of an information need. A faceted topic retrieval system must be able to cover as many facets as possible with the smallest number of documents. We introduce two novel models for faceted topic retrieval, one based on pruning a set of retrieved documents and one based on retrieving sets of documents through direct optimization of evaluation measures. We compare the performance of our models to MMR and the probabilistic model due to Zhai et al. on a set of 60 topics annotated with facets, showing that our models are competitive.\"}},{\"node\":{\"title\":\"Minimal Test Collections for Relevance Feedback\",\"details\":\"Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA, November 2009, Pages 1-6\",\"authors\":[\"B. Carterette\",\"P. Chandar\",\"A. Kailasam\",\"D. Muppaneni\",\"L. Thota\"],\"year\":\"2009\",\"month\":11,\"ptype\":\"TECHREPORT\",\"pdf\":\"carterette2009b.pdf\",\"bib\":\"carterette2009b.bib\",\"abstract\":\"The Information Retrieval Lab at the University of Delaware participated in the Relevance Feedback track at TREC 2009. We used only the Category B subset of the ClueWeb collection; our preprocessing and indexing steps are described in our paper on ad hoc and diversity runs. The second year of the Relevance Feedback track focused on selection of documents for feedback. Our hypothesis is that documents that are good at distinguishing systems in terms of their effectiveness by mean average precision will also be good documents for relevance feedback. Thus we have applied the document selection algorithm MTC (Minimal Test Collections) developed by Carterette et al.  that is used in the Million Query Track for selecting documents to be judged to find the right ranking of systems. Our approach can therefore be described as \\\"MTC for Relevance Feedback\\\".\"}},{\"node\":{\"title\":\"Implicit Feedback and Document Filtering for Retrieval Over Query Sessions\",\"details\":\"Proceedings of The Twentieth Text REtrieval Conference, TREC 2011, Gaithersburg, Maryland, USA, November 2011, Pages 1-3\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2011\",\"month\":11,\"ptype\":\"TECHREPORT\",\"pdf\":\"carterette2011b.pdf\",\"bib\":\"carterette2011b.bib\",\"abstract\":\"\"}},{\"node\":{\"title\":\"Sessions, Diversity, and Ad Hoc Retrieval\",\"details\":\"Proceedings of The Ninth Text REtrieval Conference, TREC 2010, Gaithersburg, Maryland, USA, November 2010, Pages 1-6\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2010\",\"month\":11,\"ptype\":\"TECHREPORT\",\"pdf\":\"carterette2010.pdf\",\"bib\":\"carterette2010.bib\",\"abstract\":\"\"}},{\"node\":{\"title\":\"Offline Comparative Evaluation with Incremental, Minimally-Invasive Online Feedback\",\"details\":\"Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2018, Pages to appear\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2018\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"carterette2018.pdf\",\"bib\":\"carterette2018.bib\",\"abstract\":\"We investigate the use of logged user interaction data---queries and clicks---for offline evaluation of new search systems in the context of counterfactual analysis.  The challenge of evaluating a new ranker against log data collected from a static production ranker is that new rankers may retrieve documents that have never been seen in the logs before, and thus lack any logged feedback from users.  Additionally, the ranker itself could bias user actions such that even  documents that have been seen in the logs would have exhibited different interaction patterns had they been retrieved and ranked by the new ranker.  We present a methodology for incrementally logging interactions on previously-unseen documents for use in computation of an unbiased estimator of a new ranker's effectiveness.  Our method is very lightly invasive with respect to the production ranker results to insure against users becoming dissatisfied if the new ranker is poor. We demonstrate how well our methods work in a simulation environment designed to be challenging for such methods to argue that they are likely to work in a wide variety of scenarios.\"}},{\"node\":{\"title\":\"Ad hoc and diversity retrieval at the University of Delaware\",\"details\":\"Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA, November 2009, Pages 1-8\",\"authors\":[\"P. Chandar\",\"A. Kailasam\",\"D. Muppaneni\",\"S. Thota\",\"B. Carterette\"],\"year\":\"2009\",\"month\":11,\"ptype\":\"TECHREPORT\",\"pdf\":\"chandar2009.pdf\",\"bib\":\"chandar2009.bib\",\"abstract\":\"This is the report on the University of Delaware Information Retrieval Lab's participation in the TREC 2009 Web and Million Query tracks. Our report on the Relevance Feedback track is in a separate document titled Minimal test collections for relevance feedback.\"}},{\"node\":{\"title\":\"Diversification of Search Results Using Webgraphs\",\"details\":\"Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2010, Pages 869-870\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2010\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2010.pdf\",\"bib\":\"chandar2010.bib\",\"abstract\":\"A set of words is often insufficient to express a user's information need. In order to account for various information needs associated with a query, diversification seems to be a reasonable strategy. By diversifying the result set, we increase the probability of results being relevant to the user's information needs when the given query is ambiguous. A diverse result set must contain a set of documents that cover various subtopics for a given query. We propose a graph based method which exploits the link structure of the web to return a ranked list that provides complete coverage for a query. Our method not only provides diversity to the results set, but also avoids excessive redundancy. Moreover, the probability of relevance of a document is conditioned on the documents that appear before it in the result list. We show the effectiveness of our method by comparing it with a query-likelihood model as the baseline.\"}},{\"node\":{\"title\":\"Analysis of Various Evaluation Measures for Diversity\",\"details\":\"Proceedings of the 1st International Workshop on Diversity in Document Retrieval at European Conference on Information Retrieval, April 2011, Pages 21-28\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2011\",\"month\":4,\"ptype\":\"WORKSHOP\",\"pdf\":\"chandar2011.pdf\",\"bib\":\"chandar2011.bib\",\"abstract\":\"Evaluation measures play a vital role in analyzing the performance of a system, comparing two or more systems, and optimizing systems to perform some task. In this paper, we analyze and highlight the strengths and weaknesses of commonly used measures for evaluating the diversity in search results. We compare MAP-IA, alpha-nDCG, and ERR-IA using data from TREC'09 web track diversity runs and simulated data. We describe a class of test sets that could be used to compare evaluation measure and systems used for diversifying search results.\"}},{\"node\":{\"title\":\"Using PageRank to Infer User Preferences\",\"details\":\"Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2012, Pages 1167-1168\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2012\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2012.pdf\",\"bib\":\"chandar2012.bib\",\"abstract\":\"Recently, researchers have shown interest in the use of preference judgments for evaluation in IR literature. Although preference judgments have several advantages over absolute judgment, one of the major disadvantages is that the number of judgments needed increases polynomially as the number of documents in the pool increases. We propose a novel method using PageRank to minimize the number of judgments required to evaluate systems using preference judgments. We test the proposed hypotheses using the TREC 2004 to 2006 Terabyte dataset to show that it is possible to reduce the evaluation cost considerably. Further, we study the susceptibility of the methods due to assessor errors.\"}},{\"node\":{\"title\":\"Using Preference Judgments for Novel Document Retrieval\",\"details\":\"Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2012, Pages 861-870\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2012\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2012a.pdf\",\"bib\":\"chandar2012a.bib\",\"abstract\":\"There has been considerable interest in incorporating diversity in search results to account for redundancy and the space of possible user needs. Most work on this problem is based on subtopics: diversity rankers score documents against a set of hypothesized subtopics, and diversity rankings are evaluated by assigning a value to each ranked document based on the number of novel (and redundant) subtopics it is relevant to. This can be seen as modeling a user who is always interested in seeing more novel subtopics, with progressively decreasing interest in seeing the same subtopic multiple times. We put this model to test: if it is correct, then users, when given a choice, should prefer to see a document that has more value to the evaluation. We formulate some specific hypotheses from this model and test them with actual users in a novel preference-based design in which users express a preference for document A or document B given document C. We argue that while the user study shows the subtopic model is good, there are many other factors apart from novelty and redundancy that may be influencing user preferences. From this, we introduce a new framework to construct an ideal diversity ranking using only preference judgments, with no explicit subtopic judgments whatsoever.\"}},{\"node\":{\"title\":\"What Qualities Do Users Prefer in Diversity Rankings?\",\"details\":\"Proceedings of the 2nd International Workshop on Diversity in Document Retrieval at Web Search and Data Mining Conference, February 2012, Pages 10-16\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2012\",\"month\":2,\"ptype\":\"WORKSHOP\",\"pdf\":\"chandar2012b.pdf\",\"bib\":\"chandar2012b.bib\",\"abstract\":\"\"}},{\"node\":{\"title\":\"Document Features Predicting Assessor Disagreement\",\"details\":\"Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2013, Pages 745-748\",\"authors\":[\"P. Chandar\",\"W. Webber\",\"B. Carterette\"],\"year\":\"2013\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2013.pdf\",\"bib\":\"chandar2013.bib\",\"abstract\":\"The notion of relevance differs between assessors, thus giving rise to assessor disagreement. Although assessor disagreement has been frequently observed, the factors leading to disagreement are still an open problem. In this paper we study the relationship between assessor disagreement and various topic independent factors such as readability and cohesiveness. We build a logistic model using reading level and other simple document features to predict assessor disagreement and rank documents by decreasing probability of disagreement. We compare the predictive power of these document-level features with that of a meta-search feature that aggregates a document's ranking across multiple retrieval runs. Our features are shown to be on a par with the meta-search feature, without requiring a large and diverse set of retrieval runs to calcu- late. Surprisingly, however, we find that the reading level features are negatively correlated with disagreement, suggesting that they are detecting some other aspect of document content.\"}},{\"node\":{\"title\":\"Preference Based Evaluation Measures for Novelty and Diversity\",\"details\":\"Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2013, Pages 413-422\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2013\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2013a.pdf\",\"bib\":\"chandar2013a.bib\",\"abstract\":\"Novel and diverse document ranking is an effective strategy that involves reducing redundancy in a ranked list to maximize the amount of novel and relevant information available to users. Evaluation for novelty and diversity typically involves an assessor judging each document for relevance against a set of pre-identified subtopics, which may be disambiguations of the query, facets of an information need, or nuggets of information. Alternately, when expressing a preference for document A or document B, users may implicitly take subtopics into account, but may also take into account other factors such as recency, readability, length, and so on, each of which may have more or less importance depending on user. A user profile contains information about the extent to which each factor, including subtopic relevance, plays a role in the user's preference for one document over another. A preference-based evaluation can then take this user profile information into account to better model utility to the space of users.\\nIn this work, we propose an evaluation framework that not only can consider implicit factors but also handles differences in user preference due to varying underlying information need. Our proposed framework is based on the idea that a user scanning a ranked list from top to bottom and stopping at rank k gains some utility from every document that is relevant their information need. Thus, we model the expected utility of a ranked list by estimating the utility of a document at a given rank using preference judgments and define evaluation measures based on the same. We validate our framework by comparing it to existing measures such as alpha-nDCG, ERR-IA, and subtopic recall that require explicit subtopic judgments We show that our proposed measures correlate well with existing measures while having the potential to capture various other factors when real data is used. We also show that the proposed measures can easily handle relevance assessments against multiple user profiles, and that they are robust to noisy and incomplete judgments.\"}},{\"node\":{\"title\":\"Similarity-based Recommendation of New Concepts to a Terminology\",\"details\":\"Proceedings of AMIA 2015 Annual Symposium, November 2015, Pages 386-395\",\"authors\":[\"P. Chandar\",\"A. Yaman\",\"J. Hoxha\",\"Z. He\",\"C. Weng\"],\"year\":\"2015\",\"month\":11,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2015.pdf\",\"bib\":\"chandar2015.bib\",\"abstract\":\"Terminologies can suffer from poor concept coverage due to delays in new concept insertion. This study tests a similarity-based approach to recommending concepts from a text corpus to a terminology. Our approach involves extraction of candidate concepts from a given text corpus, which are represented using a set of features. The model learns the important features to characterize a concept and recommends new concepts to a terminology. Further, we propose a cost-effect evaluation methodology to estimate the effectiveness of terminology enrichment methods. To test our methodology, we use the clinical trial eligibility criteria free-text as an example text corpus to recommend concepts for SNOMED CT. We computed precision at various rank intervals to measure the performance of the methods. Results indicate that our automated algorithm is an effective method for concept recommendation.\"}},{\"node\":{\"title\":\"Leveraging Conversational Systems to Assists New Hires During Onboarding\",\"details\":\"Human-Computer Interaction - INTERACT 2017, September 2017, Pages 381-391\",\"authors\":[\"P. Chandar\",\"Y. Khazaeni\",\"M. Davis\",\"M. Muller\",\"M. Crasso\",\"Q. Liao\",\"N. Shami\",\"W. Geyer\"],\"year\":\"2017\",\"month\":9,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2017.pdf\",\"bib\":\"chandar2017.bib\",\"abstract\":\"The task of onboarding a new hire consumes great amounts of resources from organizations. The faster a ``newbie'' becomes an ``insider'', the higher the chances of job satisfaction, retention, and advancement in their position. Conversational agents (AI agents) have the potential to effectively transform productivity in many enterprise workplace scenarios so applying them to the onboarding process can prove to be a very solid use case for such agents. In this work, we present a conversational system to aid new hires through their onboarding process. Users interact with the system via an instant messaging platform, to fulfill their work related information needs as if it were a human assistant. We describe the end-to-end process involved in building a domain specific conversational system and share our experiences in deploying it to 344 new hires in a month-long study. The feasibility of our approach is evaluated by analyzing message logs and questionnaires. Through three different measures, we observed an accuracy of about 60% at the message level and a higher than average retention rate for the agent. Our results suggest that this agent-based approach can very well compete with the existing tools for new hires.\"}},{\"node\":{\"title\":\"Estimating Clickthrough Bias in the Cascade Model\",\"details\":\"Proceedings of the 27th ACM International Conference on Information and Knowledge Management, October 2018, Pages 1587-1590\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2018\",\"month\":10,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2018.pdf\",\"bib\":\"chandar2018.bib\",\"abstract\":\"Recently, there has been considerable interest in the use of historical logged user interaction data—queries and clicks—for evaluation\\nof search systems in the context of counterfactual analysis [8, 10].\\nRecent approaches attempt to de-bias the historical log data by conducting randomization experiments and modeling the bias in user\\nbehavior. Thus far, the focus has been on addressing bias that arises\\ndue to the position of the document being clicked (position-bias) or\\nsparsity of clicks on certain query-document pairs (selection-bias).\\nHowever, there is another source of bias that could arise: the bias\\ndue to the context in which a document was presented to the user.\\nThe propensity of the user clicking on a document depends not\\nonly on its position but also on many other contextual factors.\\nIn this work, we show that the existing counterfactual estimators\\nfail to capture one type of bias, speci!cally, the e\\\"ect on clickthrough rates due to the relevance of documents ranked above.\\nFurther, we propose a modi!cation to the existing estimator that\\ntakes into account this bias. We rely on full result randomization\\nthat allows us to control for the click context at various ranks; we\\ndemonstrate the e\\\"ectiveness of our methods in evaluating retrieval\\nsystem through experiments on a simulation setup that is designed\\nto cover a wide variety of scenarios\"}},{\"node\":{\"title\":\"Developing Evaluation Metrics for Instant Search Using Mixed Methods Methods\",\"details\":\"Proceedings of the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2019, Pages 925-928\",\"authors\":[\"P. Chandar\",\"J. Garcia-Gathright\",\"C. Hosey\",\"B. St. Thomas\",\"J. Thom\"],\"year\":\"2019\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2019.pdf\",\"bib\":\"chandar2019.bib\",\"abstract\":\"Instant search has become a popular search paradigm in which users are shown a new result page in response to every keystroke triggered. Over recent years, the paradigm has been widely adopted in several domains including  personal email search, e-commerce, and music search. However, the topic of evaluation and metrics for such systems has been less explored in the literature thus far.  \\n\\nIn this work, we describe a mixed methods approach to understanding user expectations and evaluating an instant search system in the context of music search. Our methodology involves conducting a set of user interviews to gain a qualitative understanding of users' behaviors and their expectations. The hypotheses from user research are then extended and verified by a large-scale quantitative analysis of interaction logs. Using music search as a lens, we show that researchers and practitioners can interpret the behavior logs more effectively when accompanied by insights from qualitative research. We demonstrate that metrics identified using our approach are more sensitive than the commonly used click-through rate metric for instant search.\"}},{\"node\":{\"title\":\"Novelty and Diversity in Search Results\",\"details\":\"University of Delaware, August 2014, Pages 1-175\",\"authors\":[\"P. Chandar\"],\"year\":\"2014\",\"month\":8,\"ptype\":\"PHDTHESIS\",\"pdf\":\"chandarthesis2014.pdf\",\"bib\":\"chandarthesis2014.bib\",\"abstract\":\"Information retrieval (IR) is the process of obtaining relevant information for a given information need. The concept of relevance and its relation to information needs is of central concern to IR researchers. Until recently, much work in IR settled with a notion of relevance that is topical -- that is, containing information about a specified topic -- and in which the relevance of a document in a ranking is independent of the relevance of other documents in the ranking. But such an approach is more likely to produce a ranking with a high degree of redundancy; the amount of novel information available to the user may be minimal as they traverse down a ranked list.\\n\\n\\nIn this work, we focus on the novelty and diversity problem that models relevance of a document taking into account the inter-document effects in a ranked list and diverse information needs for a given query. Existing approaches to this problem mostly rely on identifying subtopics (disambiguation, facets, or other component parts) of an information need, then estimating a document's relevance independently w.r.t each subtopic. Users are treated as being satisfied by a ranking of documents that covers the space of subtopics as well as covering each individual subtopic sufficiently.\\n\\nWe propose a novel approach that models novelty implicitly while retaining the ability to capture other important factors affecting user satisfaction.\\nWe formulate a set of hypotheses based on the existing subtopic approach and test them with actual users using a simple conditional preference design: users express a preference for document A or document B given document C.\\nFollowing this, we introduce a novel triplet framework for collecting such preference judgments and using them to estimate the total utility of a document while taking inter-document effects into account.\\nFinally, a set of utility-based metrics are proposed and validated to measure the effectiveness of a system for the novelty and diversity task. \"}},{\"node\":{\"title\":\"Offline Evaluation to Make Decisions About PlaylistRecommendation Algorithms\",\"details\":\"Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, February 2019, Pages 420-428\",\"authors\":[\"A. Gruson\",\"P. Chandar\",\"C. Charbuillet\",\"J. McInerney\",\"S. Hansen\",\"D. Tardieu\",\"B. Carterette\"],\"year\":\"2019\",\"month\":2,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"gruson2019.pdf\",\"bib\":\"gruson2019.bib\",\"abstract\":\"Evaluating algorithmic recommendations is an important, but difficult, problem. Evaluations conducted offline using data collected\\nfrom user interactions with an online system often suffer from biases arising from the user interface or the recommendation engine.\\nOnline evaluation (A/B testing) can more easily address problems\\nof bias, but depending on setting can be time-consuming and incur\\nrisk of negatively impacting the user experience, not to mention\\nthat it is generally more difficult when access to a large user base is\\nnot taken as granted. A compromise based on counterfactual analysis is to present some subset of online users with recommendation\\nresults that have been randomized or otherwise manipulated, log\\ntheir interactions, and then use those to de-bias offline evaluations\\non historical data. However, previous work does not offer clear\\nconclusions on how well such methods correlate with and are able\\nto predict the results of online A/B tests. Understanding this is crucial to widespread adoption of new offline evaluation techniques in\\nrecommender systems.\\nIn this work we present a comparison of offline and online evaluation results for a particular recommendation problem: recommending playlists of tracks to a user looking for music. We describe\\ntwo different ways to think about de-biasing offline collections for\\nmore accurate evaluation. Our results show that, contrary to much\\nof the previous work on this topic, properly-conducted offline experiments do correlate well to A/B test results, and moreover that\\nwe can expect an offline evaluation to identify the best candidate\\nsystems for online testing with high probability.Evaluating algorithmic recommendations is an important, but difficult, problem. Evaluations conducted offline using data collected\\nfrom user interactions with an online system often suffer from biases arising from the user interface or the recommendation engine.\\nOnline evaluation (A/B testing) can more easily address problems\\nof bias, but depending on setting can be time-consuming and incur\\nrisk of negatively impacting the user experience, not to mention\\nthat it is generally more difficult when access to a large user base is\\nnot taken as granted. A compromise based on counterfactual analysis is to present some subset of online users with recommendation\\nresults that have been randomized or otherwise manipulated, log\\ntheir interactions, and then use those to de-bias offline evaluations\\non historical data. However, previous work does not offer clear\\nconclusions on how well such methods correlate with and are able\\nto predict the results of online A/B tests. Understanding this is crucial to widespread adoption of new offline evaluation techniques in\\nrecommender systems.\\nIn this work we present a comparison of offline and online evaluation results for a particular recommendation problem: recommending playlists of tracks to a user looking for music. We describe\\ntwo different ways to think about de-biasing offline collections for\\nmore accurate evaluation. Our results show that, contrary to much\\nof the previous work on this topic, properly-conducted offline experiments do correlate well to A/B test results, and moreover that\\nwe can expect an offline evaluation to identify the best candidate\\nsystems for online testing with high probability.\"}},{\"node\":{\"title\":\"Simulation-based Evaluation of the Generalizability Index for Study Traits\",\"details\":\"Proceedings of AMIA 2015 Annual Symposium, November 2015, Pages 593-602\",\"authors\":[\"Z. He\",\"P. Chandar\",\"P. Ryan\",\"C. Weng\"],\"year\":\"2015\",\"month\":11,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"he2015_draft.pdf\",\"bib\":\"he2015_draft.bib\",\"abstract\":\"The Generalizability Index for Study Traits (GIST) has been proposed recently for assessing the population representativeness of a set of related clinical trials using eligibility features (e.g., age or BMI), one each time. However, GIST has not yet been evaluated. To bridge this knowledge gap, this paper reports a simulation-based validation study for GIST. Using the National Health and Nutrition Examination Survey (NHANES) data, we demonstrated the effectiveness of GIST at quantifying the population representativeness of a set of related trials that differ in disease domains, study phases, sponsor types, and study designs, respectively. We also showed that among seven example medical conditions, the GIST of age increases from Phase I trials to Phase III trials in the seven disease domains and is the lowest in asthma trials. We concluded that GIST correlates with simulation-based generalizability results and is a valid metric for quantifying population representativeness of related clinical trials.\"}},{\"node\":{\"title\":\"DREAM: Classification scheme for dialog acts in clinical research query mediation\",\"details\":\"Journal of Biomedical Informatics, Volume 59, Issue 1, February 2016, Pages 89-101\",\"authors\":[\"J. Hoxha\",\"P. Chandar\",\"Z. He\",\"J. Cimino\",\"D. Hanauer\",\"C. Weng\"],\"year\":\"2016\",\"month\":2,\"ptype\":\"ARTICLE\",\"pdf\":\"hoxha2016.pdf\",\"bib\":\"hoxha2016.bib\",\"abstract\":\"Clinical data access involves complex, but opaque communication between medical researchers and query analysts. Understanding such communication is indispensable for designing intelligent human-machine dialog systems that automate query formulation. This study investigates email communication and proposes a novel scheme for classifying dialog acts in clinical research query mediation. We analyzed 315 email messages exchanged in the communication for 20 data requests obtained from three institutions. The messages were segmented into 1333 utterance units. Through a rigorous process, we developed a classification scheme and applied it for dialog act annotation of the extracted utterances. Evaluation results with high inter-annotator agreement demonstrate the reliability of this scheme. This dataset is used to contribute preliminary understanding of dialog acts distribution and conversation flow in this dialog space.\"}},{\"node\":{\"title\":\"Search Mindsets: Understanding Focused and Non-Focused Information Seeking in Music Search\",\"details\":\"The World Wide Web Conference, May 2019, Pages 2971-2977\",\"authors\":[\"A. Li\",\"J. Thom\",\"P. Chandar\",\"C. Hosey\",\"B. Thomas\",\"J. Garcia-Gathright\"],\"year\":\"2019\",\"month\":5,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"li2019.pdf\",\"bib\":\"li2019.bib\",\"abstract\":\"Music listening is a commonplace activity that has transformed\\nas users engage with online streaming platforms. When presented\\nwith anytime, anywhere access to a vast catalog of music, users face\\nchallenges in searching for what they want to hear. We propose\\nthat users who engage in domain-specific search (e.g., music search)\\nhave different information-seeking needs than in general search.\\nUsing a mixed-method approach that combines a large-scale user\\nsurvey with behavior data analyses, we describe the construct of\\nsearch mindset on a leading online streaming music platform and\\nthen investigate two types of search mindsets: focused, where a\\nuser is looking for one thing in particular, and non-focused, where\\na user is open to different results. Our results reveal that searches in\\nthe music domain are more likely to be focused than non-focused.\\nIn addition, users’ behavior (e.g., clicks, streams, querying, etc.) on\\na music search system is influenced by their search mindset. Finally,\\nwe propose design implications for music search systems to best\\nsupport their users.\"}},{\"node\":{\"title\":\"All Work and No Play? Conversations with a Question-and-Answer Chatbot in the Wild\",\"details\":\"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, April 2018, Pages 3:1-3:13\",\"authors\":[\"Q. Liao\",\"M. Mas-ud Hussain\",\"P. Chandar\",\"M. Davis\",\"Y. Khazaeni\",\"M. Crasso\",\"D. Wang\",\"M. Muller\",\"N. Shami\",\"W. Geyer\"],\"year\":\"2018\",\"month\":4,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"liao2018.pdf\",\"bib\":\"liao2018.bib\",\"abstract\":\"Many conversational agents (CAs) are developed to answer users' questions in a specialized domain. In everyday use of CAs, user experience may extend beyond satisfying information needs to the enjoyment of conversations with CAs, some of which represent playful interactions. By studying a field deployment of a Human Resource chatbot, we report on users' interest areas in conversational interactions to inform the development of CAs. Through the lens of statistical modeling, we also highlight rich signals in conversational interactions for inferring user satisfaction with the instrumental usage and playful interactions with the agent. These signals can be utilized to develop agents that adapt functionality and interaction styles. By contrasting these signals, we shed light on the varying functions of conversational interactions. We discuss design implications for CAs, and directions for developing adaptive agents based on users' conversational behaviors.\"}},{\"node\":{\"title\":\"Alternative Assessor Disagreement and Retrieval Depth\",\"details\":\"Proceedings of the 21st ACM International Conference on Information and Knowledge Management, November 2012, Pages 125-134\",\"authors\":[\"W. Webber\",\"P. Chandar\",\"B. Carterette\"],\"year\":\"2012\",\"month\":11,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"webber2012.pdf\",\"bib\":\"webber2012.bib\",\"abstract\":\"Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. In this paper, we examine the relationship between the rank at which a document is returned by a set of retrieval systems and the likelihood that assessors will disagree on its relevance, and find that there is a strong and consistent correlation between the two. We adopt a meta-rank method of summarizing a document's rank across multiple runs, and propose a logistic regression predictive model of assessor disagreement given meta-rank and initially-assessed relevance. The consistency of the model parameters across different topics, assessor pairs, and collections is considered. The model gives comparatively accurate predictions of absolute scores, but less consistent predictions of relative scores than a simpler rank-insensitive model. We demonstrate that the logistic regression model is robust to using sampled, rather than exhaustive, dual assessment. We demonstrate the use of the sampled predictive model to incorporate assessor disagreement into tests of statistical significance.\"}},{\"node\":{\"title\":\"An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings\",\"details\":\"Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2018, Pages to appear\",\"authors\":[\"D. Seyler\",\"P. Chandar\",\"M. Davis\"],\"year\":\"2018\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"syler2018.pdf\",\"bib\":\"syler2018.bib\",\"abstract\":\"We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion. Our method represents users, documents and other context-related documents as heterogeneous objects in a HIN. Using meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space. This allows inferences of user interest on unseen objects based on distance in the embedding space. These object distances are then incorporated as features in a well-established learning to rank (LTR) framework. We make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS task.\"}}]},\"allFile\":{\"edges\":[{\"node\":{\"name\":\"Chandar2012\",\"publicURL\":\"/static/Chandar2012-3b0f3697366d46256cab02306a993751.pdf\"}},{\"node\":{\"name\":\"Chandar2011\",\"publicURL\":\"/static/Chandar2011-ad54a4e6b62c94d7bd44b350b37fca40.pdf\"}},{\"node\":{\"name\":\"Chandar2010\",\"publicURL\":\"/static/Chandar2010-ec65fe73a1b19c156eefdf2ed1e243a0.pdf\"}},{\"node\":{\"name\":\"Chandar2017\",\"publicURL\":\"/static/Chandar2017-abad9c36c40017db6a9c7bf8b6099a84.pdf\"}},{\"node\":{\"name\":\"Chandar2013\",\"publicURL\":\"/static/Chandar2013-225915d6b884c50afaa74475b30c7465.pdf\"}},{\"node\":{\"name\":\"Carterette2009a\",\"publicURL\":\"/static/Carterette2009a-c61e7f0fda58817c5300703124973b71.pdf\"}},{\"node\":{\"name\":\"cv\",\"publicURL\":\"/static/cv-92ccee951fd922e51cd2ecac8d77a236.pdf\"}},{\"node\":{\"name\":\"Carterette2009\",\"publicURL\":\"/static/Carterette2009-d31169f980ed719701595fa89cfd1210.pdf\"}},{\"node\":{\"name\":\"Hruby2015\",\"publicURL\":\"/static/Hruby2015-c0d106a89788bd5658af19ab3dde5e6e.pdf\"}},{\"node\":{\"name\":\"Chandar2013a\",\"publicURL\":\"/static/Chandar2013a-f89c6a0e45a37e68206f9740f7f63d11.pdf\"}},{\"node\":{\"name\":\"Chandar2012b\",\"publicURL\":\"/static/Chandar2012b-df9f7254fb9ccbe8daa22c13d3e6ad12.pdf\"}},{\"node\":{\"name\":\"Webber2012\",\"publicURL\":\"/static/Webber2012-35ad8d83d37054b76e5f9f9f648fe1e0.pdf\"}},{\"node\":{\"name\":\"Syler2018\",\"publicURL\":\"/static/Syler2018-f70071798cae051fcd257876ef880897.pdf\"}},{\"node\":{\"name\":\"Liao2018\",\"publicURL\":\"/static/Liao2018-c9cd17b214f0908bb475c4ecd5ce6d44.pdf\"}},{\"node\":{\"name\":\"Chandar2019\",\"publicURL\":\"/static/Chandar2019-f74dd21d645fe138657315b6b64a0f89.pdf\"}},{\"node\":{\"name\":\"Carterette2018\",\"publicURL\":\"/static/Carterette2018-f7ff2c68b7a0849f12abab3d107c55da.pdf\"}},{\"node\":{\"name\":\"Gruson2019\",\"publicURL\":\"/static/Gruson2019-a2c9a8576182dcb33019d20a7c7a51b7.pdf\"}},{\"node\":{\"name\":\"Hoxha2016\",\"publicURL\":\"/static/Hoxha2016-7a1ffe1cdaaf2b3686963cc2532239e7.pdf\"}},{\"node\":{\"name\":\"Chandar2015\",\"publicURL\":\"/static/Chandar2015-2ef6dcf3800549c799bb1df433e0ac16.pdf\"}},{\"node\":{\"name\":\"Chandar2018\",\"publicURL\":\"/static/Chandar2018-158844a038c1fa3e1433db7c7b0a6e91.pdf\"}},{\"node\":{\"name\":\"Chandar2012a\",\"publicURL\":\"/static/Chandar2012a-0f453b186426d22815b23902bcd82eea.pdf\"}},{\"node\":{\"name\":\"He2015_draft\",\"publicURL\":\"/static/He2015_draft-5135ab9e13c5e961b11c30ba04788ede.pdf\"}},{\"node\":{\"name\":\"Li2019\",\"publicURL\":\"/static/Li2019-fceffca49e895e230dc7005484b84766.pdf\"}},{\"node\":{\"name\":\"ChandarThesis2014\",\"publicURL\":\"/static/ChandarThesis2014-a1910ac9ffc2fa4dd2c6a96ac9e22a71.pdf\"}}]}},\"pathContext\":{}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---publication-da588abbcfe7c0b829b2.js","module.exports = {\"data\":{\"allPubMetaJson\":{\"edges\":[{\"node\":{\"title\":\"An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings\",\"details\":\"Proceedings of the 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8-12 2018\",\"authors\":[\"D. Seyler\",\"P. Chandar\",\"M. Davis\"],\"year\":\"2018\",\"month\":null,\"ptype\":\"conference\",\"pdf\":\"sigir18b.pdf\",\"bib\":\"\",\"abstract\":\"We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion. Our method represents users, documents and other context-related documents as heterogeneous objects in a HIN. Using meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space. This allows inferences of user interest on unseen objects based on distance in the embedding space. These object distances are then incorporated as features in a well-established learning to rank (LTR) framework. We make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS task.\"}},{\"node\":{\"title\":\"Offline Comparative Evaluation with Incremental, Minimally-Invasive Online Feedback\",\"details\":\"Proceedings of the 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8-12, 2018.\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2018\",\"month\":null,\"ptype\":\"conference\",\"pdf\":\"sigir18.pdf\",\"bib\":\"sigir18.bib\",\"abstract\":\"We investigate the use of logged user interaction data---queries and clicks---for offline evaluation of new search systems in the context of counterfactual analysis.  The challenge of evaluating a new ranker against log data collected from a static production ranker is that new rankers may retrieve documents that have never been seen in the logs before, and thus lack any logged feedback from users.  Additionally, the ranker itself could bias user actions such that even  documents that have been seen in the logs would have exhibited different interaction patterns had they been retrieved and ranked by the new ranker.  We present a methodology for incrementally logging interactions on previously-unseen documents for use in computation of an unbiased estimator of a new ranker's effectiveness.  Our method is very lightly invasive with respect to the production ranker results to insure against users becoming dissatisfied if the new ranker is poor. We demonstrate how well our methods work in a simulation environment designed to be challenging for such methods to argue that they are likely to work in a wide variety of scenarios.\"}},{\"node\":{\"title\":\"Udel @ NTCIR-11 IMine Track\",\"details\":\"Proceedings of the 11th NTCIR Conference, December 2014, Pages 80-83\",\"authors\":[\"A. Bah\",\"B. Carterette\",\"P. Chandar\"],\"year\":\"2014\",\"month\":12,\"ptype\":\"TECHREPORT\",\"pdf\":\"bah2014.pdf\",\"bib\":\"bah2014.bib\",\"abstract\":\"This paper describes our participation in the Intent Mining track\\nof NTCIR-11. We present our methods and results for both\\ndocument ranking and subtopic mining. Our ranking methods are\\nbased on several data fusion techniques with some variations. Our\\nsubtopic mining method is a very simple technique that uses query\\ndimensions' items to form a subtopic.\"}},{\"node\":{\"title\":\"Document Comprehensiveness and User Preferences in Novelty Search Tasks\",\"details\":\"Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2015, Pages 735-738\",\"authors\":[\"A. Bah\",\"P. Chandar\",\"B. Carterette\"],\"year\":\"2015\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"bah2015.pdf\",\"bib\":\"bah2015.bib\",\"abstract\":\"Different users may be attempting to satisfy different information needs while providing the same query to a search engine. Addressing that issue is addressing Novelty and Diversity in information retrieval. Novelty and Diversity search task models the task wherein users are interested in seeing more and more documents that are not only relevant, but also cover more aspects (or subtopics) related to the topic of interest. This is in contrast with the traditional IR task where topical relevance is the only factor in evaluating search results. In this paper, we conduct a user study where users are asked to give a preference between one of two documents B and C given a query and also given that they have already seen a document A. We then test a total of ten hypotheses pertaining to the relationship between the \\\"comprehensiveness\\\" of documents (i.e. the number of subtopics a document is relevant to) and real users' preference judgments. Our results show that users are inclined to prefer documents with higher comprehensiveness, even when the prior document A already covers more aspects than the two documents being compared, and even when the least preferred has a higher relevance grade. In fact, users are inclined to prefer documents with higher overall aspect-coverage even in cases where B and C are relevant to the same number of novel subtopics.\"}},{\"node\":{\"title\":\"Probabilistic Models of Ranking Novel Documents for Faceted Topic Retrieval\",\"details\":\"Proceedings of the 18th ACM Conference on Information and Knowledge Management, November 2009, Pages 1287-1296\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2009\",\"month\":11,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"carterette2009.pdf\",\"bib\":\"carterette2009.bib\",\"abstract\":\"Traditional models of information retrieval assume documents are independently relevant. But when the goal is retrieving diverse or novel information about a topic, retrieval models need to capture dependencies between documents. Such tasks require alternative evaluation and optimization methods that operate on different types of relevance judgments. We define faceted topic retrieval as a particular novelty-driven task with the goal of finding a set of documents that cover the different facets of an information need. A faceted topic retrieval system must be able to cover as many facets as possible with the smallest number of documents. We introduce two novel models for faceted topic retrieval, one based on pruning a set of retrieved documents and one based on retrieving sets of documents through direct optimization of evaluation measures. We compare the performance of our models to MMR and the probabilistic model due to Zhai et al. on a set of 60 topics annotated with facets, showing that our models are competitive.\"}},{\"node\":{\"title\":\"Minimal Test Collections for Relevance Feedback\",\"details\":\"Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA, November 2009, Pages 1-6\",\"authors\":[\"B. Carterette\",\"P. Chandar\",\"A. Kailasam\",\"D. Muppaneni\",\"L. Thota\"],\"year\":\"2009\",\"month\":11,\"ptype\":\"TECHREPORT\",\"pdf\":\"carterette2009b.pdf\",\"bib\":\"carterette2009b.bib\",\"abstract\":\"The Information Retrieval Lab at the University of Delaware participated in the Relevance Feedback track at TREC 2009. We used only the Category B subset of the ClueWeb collection; our preprocessing and indexing steps are described in our paper on ad hoc and diversity runs. The second year of the Relevance Feedback track focused on selection of documents for feedback. Our hypothesis is that documents that are good at distinguishing systems in terms of their effectiveness by mean average precision will also be good documents for relevance feedback. Thus we have applied the document selection algorithm MTC (Minimal Test Collections) developed by Carterette et al.  that is used in the Million Query Track for selecting documents to be judged to find the right ranking of systems. Our approach can therefore be described as \\\"MTC for Relevance Feedback\\\".\"}},{\"node\":{\"title\":\"Implicit Feedback and Document Filtering for Retrieval Over Query Sessions\",\"details\":\"Proceedings of The Twentieth Text REtrieval Conference, TREC 2011, Gaithersburg, Maryland, USA, November 2011, Pages 1-3\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2011\",\"month\":11,\"ptype\":\"TECHREPORT\",\"pdf\":\"carterette2011b.pdf\",\"bib\":\"carterette2011b.bib\",\"abstract\":\"\"}},{\"node\":{\"title\":\"Sessions, Diversity, and Ad Hoc Retrieval\",\"details\":\"Proceedings of The Ninth Text REtrieval Conference, TREC 2010, Gaithersburg, Maryland, USA, November 2010, Pages 1-6\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2010\",\"month\":11,\"ptype\":\"TECHREPORT\",\"pdf\":\"carterette2010.pdf\",\"bib\":\"carterette2010.bib\",\"abstract\":\"\"}},{\"node\":{\"title\":\"Offline Comparative Evaluation with Incremental, Minimally-Invasive Online Feedback\",\"details\":\"Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2018, Pages to appear\",\"authors\":[\"B. Carterette\",\"P. Chandar\"],\"year\":\"2018\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"carterette2018.pdf\",\"bib\":\"carterette2018.bib\",\"abstract\":\"We investigate the use of logged user interaction data---queries and clicks---for offline evaluation of new search systems in the context of counterfactual analysis.  The challenge of evaluating a new ranker against log data collected from a static production ranker is that new rankers may retrieve documents that have never been seen in the logs before, and thus lack any logged feedback from users.  Additionally, the ranker itself could bias user actions such that even  documents that have been seen in the logs would have exhibited different interaction patterns had they been retrieved and ranked by the new ranker.  We present a methodology for incrementally logging interactions on previously-unseen documents for use in computation of an unbiased estimator of a new ranker's effectiveness.  Our method is very lightly invasive with respect to the production ranker results to insure against users becoming dissatisfied if the new ranker is poor. We demonstrate how well our methods work in a simulation environment designed to be challenging for such methods to argue that they are likely to work in a wide variety of scenarios.\"}},{\"node\":{\"title\":\"Ad hoc and diversity retrieval at the University of Delaware\",\"details\":\"Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA, November 2009, Pages 1-8\",\"authors\":[\"P. Chandar\",\"A. Kailasam\",\"D. Muppaneni\",\"S. Thota\",\"B. Carterette\"],\"year\":\"2009\",\"month\":11,\"ptype\":\"TECHREPORT\",\"pdf\":\"chandar2009.pdf\",\"bib\":\"chandar2009.bib\",\"abstract\":\"This is the report on the University of Delaware Information Retrieval Lab's participation in the TREC 2009 Web and Million Query tracks. Our report on the Relevance Feedback track is in a separate document titled Minimal test collections for relevance feedback.\"}},{\"node\":{\"title\":\"Diversification of Search Results Using Webgraphs\",\"details\":\"Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2010, Pages 869-870\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2010\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2010.pdf\",\"bib\":\"chandar2010.bib\",\"abstract\":\"A set of words is often insufficient to express a user's information need. In order to account for various information needs associated with a query, diversification seems to be a reasonable strategy. By diversifying the result set, we increase the probability of results being relevant to the user's information needs when the given query is ambiguous. A diverse result set must contain a set of documents that cover various subtopics for a given query. We propose a graph based method which exploits the link structure of the web to return a ranked list that provides complete coverage for a query. Our method not only provides diversity to the results set, but also avoids excessive redundancy. Moreover, the probability of relevance of a document is conditioned on the documents that appear before it in the result list. We show the effectiveness of our method by comparing it with a query-likelihood model as the baseline.\"}},{\"node\":{\"title\":\"Analysis of Various Evaluation Measures for Diversity\",\"details\":\"Proceedings of the 1st International Workshop on Diversity in Document Retrieval at European Conference on Information Retrieval, April 2011, Pages 21-28\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2011\",\"month\":4,\"ptype\":\"WORKSHOP\",\"pdf\":\"chandar2011.pdf\",\"bib\":\"chandar2011.bib\",\"abstract\":\"Evaluation measures play a vital role in analyzing the performance of a system, comparing two or more systems, and optimizing systems to perform some task. In this paper, we analyze and highlight the strengths and weaknesses of commonly used measures for evaluating the diversity in search results. We compare MAP-IA, alpha-nDCG, and ERR-IA using data from TREC'09 web track diversity runs and simulated data. We describe a class of test sets that could be used to compare evaluation measure and systems used for diversifying search results.\"}},{\"node\":{\"title\":\"Using PageRank to Infer User Preferences\",\"details\":\"Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2012, Pages 1167-1168\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2012\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2012.pdf\",\"bib\":\"chandar2012.bib\",\"abstract\":\"Recently, researchers have shown interest in the use of preference judgments for evaluation in IR literature. Although preference judgments have several advantages over absolute judgment, one of the major disadvantages is that the number of judgments needed increases polynomially as the number of documents in the pool increases. We propose a novel method using PageRank to minimize the number of judgments required to evaluate systems using preference judgments. We test the proposed hypotheses using the TREC 2004 to 2006 Terabyte dataset to show that it is possible to reduce the evaluation cost considerably. Further, we study the susceptibility of the methods due to assessor errors.\"}},{\"node\":{\"title\":\"Using Preference Judgments for Novel Document Retrieval\",\"details\":\"Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2012, Pages 861-870\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2012\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2012a.pdf\",\"bib\":\"chandar2012a.bib\",\"abstract\":\"There has been considerable interest in incorporating diversity in search results to account for redundancy and the space of possible user needs. Most work on this problem is based on subtopics: diversity rankers score documents against a set of hypothesized subtopics, and diversity rankings are evaluated by assigning a value to each ranked document based on the number of novel (and redundant) subtopics it is relevant to. This can be seen as modeling a user who is always interested in seeing more novel subtopics, with progressively decreasing interest in seeing the same subtopic multiple times. We put this model to test: if it is correct, then users, when given a choice, should prefer to see a document that has more value to the evaluation. We formulate some specific hypotheses from this model and test them with actual users in a novel preference-based design in which users express a preference for document A or document B given document C. We argue that while the user study shows the subtopic model is good, there are many other factors apart from novelty and redundancy that may be influencing user preferences. From this, we introduce a new framework to construct an ideal diversity ranking using only preference judgments, with no explicit subtopic judgments whatsoever.\"}},{\"node\":{\"title\":\"What Qualities Do Users Prefer in Diversity Rankings?\",\"details\":\"Proceedings of the 2nd International Workshop on Diversity in Document Retrieval at Web Search and Data Mining Conference, February 2012, Pages 10-16\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2012\",\"month\":2,\"ptype\":\"WORKSHOP\",\"pdf\":\"chandar2012b.pdf\",\"bib\":\"chandar2012b.bib\",\"abstract\":\"\"}},{\"node\":{\"title\":\"Document Features Predicting Assessor Disagreement\",\"details\":\"Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2013, Pages 745-748\",\"authors\":[\"P. Chandar\",\"W. Webber\",\"B. Carterette\"],\"year\":\"2013\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2013.pdf\",\"bib\":\"chandar2013.bib\",\"abstract\":\"The notion of relevance differs between assessors, thus giving rise to assessor disagreement. Although assessor disagreement has been frequently observed, the factors leading to disagreement are still an open problem. In this paper we study the relationship between assessor disagreement and various topic independent factors such as readability and cohesiveness. We build a logistic model using reading level and other simple document features to predict assessor disagreement and rank documents by decreasing probability of disagreement. We compare the predictive power of these document-level features with that of a meta-search feature that aggregates a document's ranking across multiple retrieval runs. Our features are shown to be on a par with the meta-search feature, without requiring a large and diverse set of retrieval runs to calcu- late. Surprisingly, however, we find that the reading level features are negatively correlated with disagreement, suggesting that they are detecting some other aspect of document content.\"}},{\"node\":{\"title\":\"Preference Based Evaluation Measures for Novelty and Diversity\",\"details\":\"Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2013, Pages 413-422\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2013\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2013a.pdf\",\"bib\":\"chandar2013a.bib\",\"abstract\":\"Novel and diverse document ranking is an effective strategy that involves reducing redundancy in a ranked list to maximize the amount of novel and relevant information available to users. Evaluation for novelty and diversity typically involves an assessor judging each document for relevance against a set of pre-identified subtopics, which may be disambiguations of the query, facets of an information need, or nuggets of information. Alternately, when expressing a preference for document A or document B, users may implicitly take subtopics into account, but may also take into account other factors such as recency, readability, length, and so on, each of which may have more or less importance depending on user. A user profile contains information about the extent to which each factor, including subtopic relevance, plays a role in the user's preference for one document over another. A preference-based evaluation can then take this user profile information into account to better model utility to the space of users.\\nIn this work, we propose an evaluation framework that not only can consider implicit factors but also handles differences in user preference due to varying underlying information need. Our proposed framework is based on the idea that a user scanning a ranked list from top to bottom and stopping at rank k gains some utility from every document that is relevant their information need. Thus, we model the expected utility of a ranked list by estimating the utility of a document at a given rank using preference judgments and define evaluation measures based on the same. We validate our framework by comparing it to existing measures such as alpha-nDCG, ERR-IA, and subtopic recall that require explicit subtopic judgments We show that our proposed measures correlate well with existing measures while having the potential to capture various other factors when real data is used. We also show that the proposed measures can easily handle relevance assessments against multiple user profiles, and that they are robust to noisy and incomplete judgments.\"}},{\"node\":{\"title\":\"Similarity-based Recommendation of New Concepts to a Terminology\",\"details\":\"Proceedings of AMIA 2015 Annual Symposium, November 2015, Pages 386-395\",\"authors\":[\"P. Chandar\",\"A. Yaman\",\"J. Hoxha\",\"Z. He\",\"C. Weng\"],\"year\":\"2015\",\"month\":11,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2015.pdf\",\"bib\":\"chandar2015.bib\",\"abstract\":\"Terminologies can suffer from poor concept coverage due to delays in new concept insertion. This study tests a similarity-based approach to recommending concepts from a text corpus to a terminology. Our approach involves extraction of candidate concepts from a given text corpus, which are represented using a set of features. The model learns the important features to characterize a concept and recommends new concepts to a terminology. Further, we propose a cost-effect evaluation methodology to estimate the effectiveness of terminology enrichment methods. To test our methodology, we use the clinical trial eligibility criteria free-text as an example text corpus to recommend concepts for SNOMED CT. We computed precision at various rank intervals to measure the performance of the methods. Results indicate that our automated algorithm is an effective method for concept recommendation.\"}},{\"node\":{\"title\":\"Leveraging Conversational Systems to Assists New Hires During Onboarding\",\"details\":\"Human-Computer Interaction - INTERACT 2017, September 2017, Pages 381-391\",\"authors\":[\"P. Chandar\",\"Y. Khazaeni\",\"M. Davis\",\"M. Muller\",\"M. Crasso\",\"Q. Liao\",\"N. Shami\",\"W. Geyer\"],\"year\":\"2017\",\"month\":9,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2017.pdf\",\"bib\":\"chandar2017.bib\",\"abstract\":\"The task of onboarding a new hire consumes great amounts of resources from organizations. The faster a ``newbie'' becomes an ``insider'', the higher the chances of job satisfaction, retention, and advancement in their position. Conversational agents (AI agents) have the potential to effectively transform productivity in many enterprise workplace scenarios so applying them to the onboarding process can prove to be a very solid use case for such agents. In this work, we present a conversational system to aid new hires through their onboarding process. Users interact with the system via an instant messaging platform, to fulfill their work related information needs as if it were a human assistant. We describe the end-to-end process involved in building a domain specific conversational system and share our experiences in deploying it to 344 new hires in a month-long study. The feasibility of our approach is evaluated by analyzing message logs and questionnaires. Through three different measures, we observed an accuracy of about 60% at the message level and a higher than average retention rate for the agent. Our results suggest that this agent-based approach can very well compete with the existing tools for new hires.\"}},{\"node\":{\"title\":\"Estimating Clickthrough Bias in the Cascade Model\",\"details\":\"Proceedings of the 27th ACM International Conference on Information and Knowledge Management, October 2018, Pages 1587-1590\",\"authors\":[\"P. Chandar\",\"B. Carterette\"],\"year\":\"2018\",\"month\":10,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2018.pdf\",\"bib\":\"chandar2018.bib\",\"abstract\":\"Recently, there has been considerable interest in the use of historical logged user interaction data—queries and clicks—for evaluation\\nof search systems in the context of counterfactual analysis [8, 10].\\nRecent approaches attempt to de-bias the historical log data by conducting randomization experiments and modeling the bias in user\\nbehavior. Thus far, the focus has been on addressing bias that arises\\ndue to the position of the document being clicked (position-bias) or\\nsparsity of clicks on certain query-document pairs (selection-bias).\\nHowever, there is another source of bias that could arise: the bias\\ndue to the context in which a document was presented to the user.\\nThe propensity of the user clicking on a document depends not\\nonly on its position but also on many other contextual factors.\\nIn this work, we show that the existing counterfactual estimators\\nfail to capture one type of bias, speci!cally, the e\\\"ect on clickthrough rates due to the relevance of documents ranked above.\\nFurther, we propose a modi!cation to the existing estimator that\\ntakes into account this bias. We rely on full result randomization\\nthat allows us to control for the click context at various ranks; we\\ndemonstrate the e\\\"ectiveness of our methods in evaluating retrieval\\nsystem through experiments on a simulation setup that is designed\\nto cover a wide variety of scenarios\"}},{\"node\":{\"title\":\"Developing Evaluation Metrics for Instant Search Using Mixed Methods Methods\",\"details\":\"Proceedings of the 42Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2019, Pages 925-928\",\"authors\":[\"P. Chandar\",\"J. Garcia-Gathright\",\"C. Hosey\",\"B. St. Thomas\",\"J. Thom\"],\"year\":\"2019\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"chandar2019.pdf\",\"bib\":\"chandar2019.bib\",\"abstract\":\"Instant search has become a popular search paradigm in which users are shown a new result page in response to every keystroke triggered. Over recent years, the paradigm has been widely adopted in several domains including  personal email search, e-commerce, and music search. However, the topic of evaluation and metrics for such systems has been less explored in the literature thus far.  \\n\\nIn this work, we describe a mixed methods approach to understanding user expectations and evaluating an instant search system in the context of music search. Our methodology involves conducting a set of user interviews to gain a qualitative understanding of users' behaviors and their expectations. The hypotheses from user research are then extended and verified by a large-scale quantitative analysis of interaction logs. Using music search as a lens, we show that researchers and practitioners can interpret the behavior logs more effectively when accompanied by insights from qualitative research. We demonstrate that metrics identified using our approach are more sensitive than the commonly used click-through rate metric for instant search.\"}},{\"node\":{\"title\":\"Novelty and Diversity in Search Results\",\"details\":\"University of Delaware, August 2014, Pages 1-175\",\"authors\":[\"P. Chandar\"],\"year\":\"2014\",\"month\":8,\"ptype\":\"PHDTHESIS\",\"pdf\":\"chandarthesis2014.pdf\",\"bib\":\"chandarthesis2014.bib\",\"abstract\":\"Information retrieval (IR) is the process of obtaining relevant information for a given information need. The concept of relevance and its relation to information needs is of central concern to IR researchers. Until recently, much work in IR settled with a notion of relevance that is topical -- that is, containing information about a specified topic -- and in which the relevance of a document in a ranking is independent of the relevance of other documents in the ranking. But such an approach is more likely to produce a ranking with a high degree of redundancy; the amount of novel information available to the user may be minimal as they traverse down a ranked list.\\n\\n\\nIn this work, we focus on the novelty and diversity problem that models relevance of a document taking into account the inter-document effects in a ranked list and diverse information needs for a given query. Existing approaches to this problem mostly rely on identifying subtopics (disambiguation, facets, or other component parts) of an information need, then estimating a document's relevance independently w.r.t each subtopic. Users are treated as being satisfied by a ranking of documents that covers the space of subtopics as well as covering each individual subtopic sufficiently.\\n\\nWe propose a novel approach that models novelty implicitly while retaining the ability to capture other important factors affecting user satisfaction.\\nWe formulate a set of hypotheses based on the existing subtopic approach and test them with actual users using a simple conditional preference design: users express a preference for document A or document B given document C.\\nFollowing this, we introduce a novel triplet framework for collecting such preference judgments and using them to estimate the total utility of a document while taking inter-document effects into account.\\nFinally, a set of utility-based metrics are proposed and validated to measure the effectiveness of a system for the novelty and diversity task. \"}},{\"node\":{\"title\":\"Offline Evaluation to Make Decisions About PlaylistRecommendation Algorithms\",\"details\":\"Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, February 2019, Pages 420-428\",\"authors\":[\"A. Gruson\",\"P. Chandar\",\"C. Charbuillet\",\"J. McInerney\",\"S. Hansen\",\"D. Tardieu\",\"B. Carterette\"],\"year\":\"2019\",\"month\":2,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"gruson2019.pdf\",\"bib\":\"gruson2019.bib\",\"abstract\":\"Evaluating algorithmic recommendations is an important, but difficult, problem. Evaluations conducted offline using data collected\\nfrom user interactions with an online system often suffer from biases arising from the user interface or the recommendation engine.\\nOnline evaluation (A/B testing) can more easily address problems\\nof bias, but depending on setting can be time-consuming and incur\\nrisk of negatively impacting the user experience, not to mention\\nthat it is generally more difficult when access to a large user base is\\nnot taken as granted. A compromise based on counterfactual analysis is to present some subset of online users with recommendation\\nresults that have been randomized or otherwise manipulated, log\\ntheir interactions, and then use those to de-bias offline evaluations\\non historical data. However, previous work does not offer clear\\nconclusions on how well such methods correlate with and are able\\nto predict the results of online A/B tests. Understanding this is crucial to widespread adoption of new offline evaluation techniques in\\nrecommender systems.\\nIn this work we present a comparison of offline and online evaluation results for a particular recommendation problem: recommending playlists of tracks to a user looking for music. We describe\\ntwo different ways to think about de-biasing offline collections for\\nmore accurate evaluation. Our results show that, contrary to much\\nof the previous work on this topic, properly-conducted offline experiments do correlate well to A/B test results, and moreover that\\nwe can expect an offline evaluation to identify the best candidate\\nsystems for online testing with high probability.Evaluating algorithmic recommendations is an important, but difficult, problem. Evaluations conducted offline using data collected\\nfrom user interactions with an online system often suffer from biases arising from the user interface or the recommendation engine.\\nOnline evaluation (A/B testing) can more easily address problems\\nof bias, but depending on setting can be time-consuming and incur\\nrisk of negatively impacting the user experience, not to mention\\nthat it is generally more difficult when access to a large user base is\\nnot taken as granted. A compromise based on counterfactual analysis is to present some subset of online users with recommendation\\nresults that have been randomized or otherwise manipulated, log\\ntheir interactions, and then use those to de-bias offline evaluations\\non historical data. However, previous work does not offer clear\\nconclusions on how well such methods correlate with and are able\\nto predict the results of online A/B tests. Understanding this is crucial to widespread adoption of new offline evaluation techniques in\\nrecommender systems.\\nIn this work we present a comparison of offline and online evaluation results for a particular recommendation problem: recommending playlists of tracks to a user looking for music. We describe\\ntwo different ways to think about de-biasing offline collections for\\nmore accurate evaluation. Our results show that, contrary to much\\nof the previous work on this topic, properly-conducted offline experiments do correlate well to A/B test results, and moreover that\\nwe can expect an offline evaluation to identify the best candidate\\nsystems for online testing with high probability.\"}},{\"node\":{\"title\":\"Simulation-based Evaluation of the Generalizability Index for Study Traits\",\"details\":\"Proceedings of AMIA 2015 Annual Symposium, November 2015, Pages 593-602\",\"authors\":[\"Z. He\",\"P. Chandar\",\"P. Ryan\",\"C. Weng\"],\"year\":\"2015\",\"month\":11,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"he2015_draft.pdf\",\"bib\":\"he2015_draft.bib\",\"abstract\":\"The Generalizability Index for Study Traits (GIST) has been proposed recently for assessing the population representativeness of a set of related clinical trials using eligibility features (e.g., age or BMI), one each time. However, GIST has not yet been evaluated. To bridge this knowledge gap, this paper reports a simulation-based validation study for GIST. Using the National Health and Nutrition Examination Survey (NHANES) data, we demonstrated the effectiveness of GIST at quantifying the population representativeness of a set of related trials that differ in disease domains, study phases, sponsor types, and study designs, respectively. We also showed that among seven example medical conditions, the GIST of age increases from Phase I trials to Phase III trials in the seven disease domains and is the lowest in asthma trials. We concluded that GIST correlates with simulation-based generalizability results and is a valid metric for quantifying population representativeness of related clinical trials.\"}},{\"node\":{\"title\":\"DREAM: Classification scheme for dialog acts in clinical research query mediation\",\"details\":\"Journal of Biomedical Informatics, Volume 59, Issue 1, February 2016, Pages 89-101\",\"authors\":[\"J. Hoxha\",\"P. Chandar\",\"Z. He\",\"J. Cimino\",\"D. Hanauer\",\"C. Weng\"],\"year\":\"2016\",\"month\":2,\"ptype\":\"ARTICLE\",\"pdf\":\"hoxha2016.pdf\",\"bib\":\"hoxha2016.bib\",\"abstract\":\"Clinical data access involves complex, but opaque communication between medical researchers and query analysts. Understanding such communication is indispensable for designing intelligent human-machine dialog systems that automate query formulation. This study investigates email communication and proposes a novel scheme for classifying dialog acts in clinical research query mediation. We analyzed 315 email messages exchanged in the communication for 20 data requests obtained from three institutions. The messages were segmented into 1333 utterance units. Through a rigorous process, we developed a classification scheme and applied it for dialog act annotation of the extracted utterances. Evaluation results with high inter-annotator agreement demonstrate the reliability of this scheme. This dataset is used to contribute preliminary understanding of dialog acts distribution and conversation flow in this dialog space.\"}},{\"node\":{\"title\":\"Search Mindsets: Understanding Focused and Non-Focused Information Seeking in Music Search\",\"details\":\"The World Wide Web Conference, May 2019, Pages 2971-2977\",\"authors\":[\"A. Li\",\"J. Thom\",\"P. Chandar\",\"C. Hosey\",\"B. Thomas\",\"J. Garcia-Gathright\"],\"year\":\"2019\",\"month\":5,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"li2019.pdf\",\"bib\":\"li2019.bib\",\"abstract\":\"Music listening is a commonplace activity that has transformed\\nas users engage with online streaming platforms. When presented\\nwith anytime, anywhere access to a vast catalog of music, users face\\nchallenges in searching for what they want to hear. We propose\\nthat users who engage in domain-specific search (e.g., music search)\\nhave different information-seeking needs than in general search.\\nUsing a mixed-method approach that combines a large-scale user\\nsurvey with behavior data analyses, we describe the construct of\\nsearch mindset on a leading online streaming music platform and\\nthen investigate two types of search mindsets: focused, where a\\nuser is looking for one thing in particular, and non-focused, where\\na user is open to different results. Our results reveal that searches in\\nthe music domain are more likely to be focused than non-focused.\\nIn addition, users’ behavior (e.g., clicks, streams, querying, etc.) on\\na music search system is influenced by their search mindset. Finally,\\nwe propose design implications for music search systems to best\\nsupport their users.\"}},{\"node\":{\"title\":\"All Work and No Play? Conversations with a Question-and-Answer Chatbot in the Wild\",\"details\":\"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, April 2018, Pages 3:1-3:13\",\"authors\":[\"Q. Liao\",\"M. Mas-ud Hussain\",\"P. Chandar\",\"M. Davis\",\"Y. Khazaeni\",\"M. Crasso\",\"D. Wang\",\"M. Muller\",\"N. Shami\",\"W. Geyer\"],\"year\":\"2018\",\"month\":4,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"liao2018.pdf\",\"bib\":\"liao2018.bib\",\"abstract\":\"Many conversational agents (CAs) are developed to answer users' questions in a specialized domain. In everyday use of CAs, user experience may extend beyond satisfying information needs to the enjoyment of conversations with CAs, some of which represent playful interactions. By studying a field deployment of a Human Resource chatbot, we report on users' interest areas in conversational interactions to inform the development of CAs. Through the lens of statistical modeling, we also highlight rich signals in conversational interactions for inferring user satisfaction with the instrumental usage and playful interactions with the agent. These signals can be utilized to develop agents that adapt functionality and interaction styles. By contrasting these signals, we shed light on the varying functions of conversational interactions. We discuss design implications for CAs, and directions for developing adaptive agents based on users' conversational behaviors.\"}},{\"node\":{\"title\":\"Alternative Assessor Disagreement and Retrieval Depth\",\"details\":\"Proceedings of the 21st ACM International Conference on Information and Knowledge Management, November 2012, Pages 125-134\",\"authors\":[\"W. Webber\",\"P. Chandar\",\"B. Carterette\"],\"year\":\"2012\",\"month\":11,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"webber2012.pdf\",\"bib\":\"webber2012.bib\",\"abstract\":\"Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. In this paper, we examine the relationship between the rank at which a document is returned by a set of retrieval systems and the likelihood that assessors will disagree on its relevance, and find that there is a strong and consistent correlation between the two. We adopt a meta-rank method of summarizing a document's rank across multiple runs, and propose a logistic regression predictive model of assessor disagreement given meta-rank and initially-assessed relevance. The consistency of the model parameters across different topics, assessor pairs, and collections is considered. The model gives comparatively accurate predictions of absolute scores, but less consistent predictions of relative scores than a simpler rank-insensitive model. We demonstrate that the logistic regression model is robust to using sampled, rather than exhaustive, dual assessment. We demonstrate the use of the sampled predictive model to incorporate assessor disagreement into tests of statistical significance.\"}},{\"node\":{\"title\":\"An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings\",\"details\":\"Proceedings of the 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 2018, Pages to appear\",\"authors\":[\"D. Seyler\",\"P. Chandar\",\"M. Davis\"],\"year\":\"2018\",\"month\":7,\"ptype\":\"INPROCEEDINGS\",\"pdf\":\"syler2018.pdf\",\"bib\":\"syler2018.bib\",\"abstract\":\"We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion. Our method represents users, documents and other context-related documents as heterogeneous objects in a HIN. Using meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space. This allows inferences of user interest on unseen objects based on distance in the embedding space. These object distances are then incorporated as features in a well-established learning to rank (LTR) framework. We make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS task.\"}}]},\"allFile\":{\"edges\":[{\"node\":{\"name\":\"Chandar2012\",\"publicURL\":\"/static/Chandar2012-3b0f3697366d46256cab02306a993751.pdf\"}},{\"node\":{\"name\":\"Chandar2011\",\"publicURL\":\"/static/Chandar2011-ad54a4e6b62c94d7bd44b350b37fca40.pdf\"}},{\"node\":{\"name\":\"Chandar2010\",\"publicURL\":\"/static/Chandar2010-ec65fe73a1b19c156eefdf2ed1e243a0.pdf\"}},{\"node\":{\"name\":\"Chandar2017\",\"publicURL\":\"/static/Chandar2017-abad9c36c40017db6a9c7bf8b6099a84.pdf\"}},{\"node\":{\"name\":\"Chandar2013\",\"publicURL\":\"/static/Chandar2013-225915d6b884c50afaa74475b30c7465.pdf\"}},{\"node\":{\"name\":\"Carterette2009a\",\"publicURL\":\"/static/Carterette2009a-c61e7f0fda58817c5300703124973b71.pdf\"}},{\"node\":{\"name\":\"cv\",\"publicURL\":\"/static/cv-92ccee951fd922e51cd2ecac8d77a236.pdf\"}},{\"node\":{\"name\":\"Carterette2009\",\"publicURL\":\"/static/Carterette2009-d31169f980ed719701595fa89cfd1210.pdf\"}},{\"node\":{\"name\":\"Hruby2015\",\"publicURL\":\"/static/Hruby2015-c0d106a89788bd5658af19ab3dde5e6e.pdf\"}},{\"node\":{\"name\":\"Chandar2013a\",\"publicURL\":\"/static/Chandar2013a-f89c6a0e45a37e68206f9740f7f63d11.pdf\"}},{\"node\":{\"name\":\"Chandar2012b\",\"publicURL\":\"/static/Chandar2012b-df9f7254fb9ccbe8daa22c13d3e6ad12.pdf\"}},{\"node\":{\"name\":\"Webber2012\",\"publicURL\":\"/static/Webber2012-35ad8d83d37054b76e5f9f9f648fe1e0.pdf\"}},{\"node\":{\"name\":\"Syler2018\",\"publicURL\":\"/static/Syler2018-f70071798cae051fcd257876ef880897.pdf\"}},{\"node\":{\"name\":\"Liao2018\",\"publicURL\":\"/static/Liao2018-c9cd17b214f0908bb475c4ecd5ce6d44.pdf\"}},{\"node\":{\"name\":\"Chandar2019\",\"publicURL\":\"/static/Chandar2019-f74dd21d645fe138657315b6b64a0f89.pdf\"}},{\"node\":{\"name\":\"Carterette2018\",\"publicURL\":\"/static/Carterette2018-f7ff2c68b7a0849f12abab3d107c55da.pdf\"}},{\"node\":{\"name\":\"Gruson2019\",\"publicURL\":\"/static/Gruson2019-a2c9a8576182dcb33019d20a7c7a51b7.pdf\"}},{\"node\":{\"name\":\"Hoxha2016\",\"publicURL\":\"/static/Hoxha2016-7a1ffe1cdaaf2b3686963cc2532239e7.pdf\"}},{\"node\":{\"name\":\"Chandar2015\",\"publicURL\":\"/static/Chandar2015-2ef6dcf3800549c799bb1df433e0ac16.pdf\"}},{\"node\":{\"name\":\"Chandar2018\",\"publicURL\":\"/static/Chandar2018-158844a038c1fa3e1433db7c7b0a6e91.pdf\"}},{\"node\":{\"name\":\"Chandar2012a\",\"publicURL\":\"/static/Chandar2012a-0f453b186426d22815b23902bcd82eea.pdf\"}},{\"node\":{\"name\":\"He2015_draft\",\"publicURL\":\"/static/He2015_draft-5135ab9e13c5e961b11c30ba04788ede.pdf\"}},{\"node\":{\"name\":\"Li2019\",\"publicURL\":\"/static/Li2019-fceffca49e895e230dc7005484b84766.pdf\"}},{\"node\":{\"name\":\"ChandarThesis2014\",\"publicURL\":\"/static/ChandarThesis2014-a1910ac9ffc2fa4dd2c6a96ac9e22a71.pdf\"}}]}},\"pathContext\":{}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/publication.json\n// module id = 276\n// module chunks = 141926880579864"],"sourceRoot":""}