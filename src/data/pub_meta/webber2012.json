{"title":"Alternative Assessor Disagreement and Retrieval Depth","authors":["W. Webber","P. Chandar","B. Carterette"],"year":"2012","month":11,"details":"Proceedings of the 21st ACM International Conference on Information and Knowledge Management, November 2012, Pages 125-134","abstract":"Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. In this paper, we examine the relationship between the rank at which a document is returned by a set of retrieval systems and the likelihood that assessors will disagree on its relevance, and find that there is a strong and consistent correlation between the two. We adopt a meta-rank method of summarizing a document's rank across multiple runs, and propose a logistic regression predictive model of assessor disagreement given meta-rank and initially-assessed relevance. The consistency of the model parameters across different topics, assessor pairs, and collections is considered. The model gives comparatively accurate predictions of absolute scores, but less consistent predictions of relative scores than a simpler rank-insensitive model. We demonstrate that the logistic regression model is robust to using sampled, rather than exhaustive, dual assessment. We demonstrate the use of the sampled predictive model to incorporate assessor disagreement into tests of statistical significance.","ptype":"INPROCEEDINGS","pdf":"webber2012.pdf","bib":"webber2012.bib","meta":"webber2012.json"}