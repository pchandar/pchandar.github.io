{"title":"Offline Evaluation to Make Decisions About PlaylistRecommendation Algorithms","authors":["A. Gruson","P. Chandar","C. Charbuillet","J. McInerney","S. Hansen","D. Tardieu","B. Carterette"],"year":"2019","month":2,"details":"Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, February 2019, Pages 420-428","abstract":"Evaluating algorithmic recommendations is an important, but difficult, problem. Evaluations conducted offline using data collected\nfrom user interactions with an online system often suffer from biases arising from the user interface or the recommendation engine.\nOnline evaluation (A/B testing) can more easily address problems\nof bias, but depending on setting can be time-consuming and incur\nrisk of negatively impacting the user experience, not to mention\nthat it is generally more difficult when access to a large user base is\nnot taken as granted. A compromise based on counterfactual analysis is to present some subset of online users with recommendation\nresults that have been randomized or otherwise manipulated, log\ntheir interactions, and then use those to de-bias offline evaluations\non historical data. However, previous work does not offer clear\nconclusions on how well such methods correlate with and are able\nto predict the results of online A/B tests. Understanding this is crucial to widespread adoption of new offline evaluation techniques in\nrecommender systems.\nIn this work we present a comparison of offline and online evaluation results for a particular recommendation problem: recommending playlists of tracks to a user looking for music. We describe\ntwo different ways to think about de-biasing offline collections for\nmore accurate evaluation. Our results show that, contrary to much\nof the previous work on this topic, properly-conducted offline experiments do correlate well to A/B test results, and moreover that\nwe can expect an offline evaluation to identify the best candidate\nsystems for online testing with high probability.Evaluating algorithmic recommendations is an important, but difficult, problem. Evaluations conducted offline using data collected\nfrom user interactions with an online system often suffer from biases arising from the user interface or the recommendation engine.\nOnline evaluation (A/B testing) can more easily address problems\nof bias, but depending on setting can be time-consuming and incur\nrisk of negatively impacting the user experience, not to mention\nthat it is generally more difficult when access to a large user base is\nnot taken as granted. A compromise based on counterfactual analysis is to present some subset of online users with recommendation\nresults that have been randomized or otherwise manipulated, log\ntheir interactions, and then use those to de-bias offline evaluations\non historical data. However, previous work does not offer clear\nconclusions on how well such methods correlate with and are able\nto predict the results of online A/B tests. Understanding this is crucial to widespread adoption of new offline evaluation techniques in\nrecommender systems.\nIn this work we present a comparison of offline and online evaluation results for a particular recommendation problem: recommending playlists of tracks to a user looking for music. We describe\ntwo different ways to think about de-biasing offline collections for\nmore accurate evaluation. Our results show that, contrary to much\nof the previous work on this topic, properly-conducted offline experiments do correlate well to A/B test results, and moreover that\nwe can expect an offline evaluation to identify the best candidate\nsystems for online testing with high probability.","ptype":"INPROCEEDINGS","pdf":"gruson2019.pdf","bib":"gruson2019.bib","meta":"gruson2019.json"}