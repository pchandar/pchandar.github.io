{"title":"Minimal Test Collections for Relevance Feedback","authors":["B. Carterette","P. Chandar","A. Kailasam","D. Muppaneni","L. Thota"],"year":"2009","month":11,"details":"Proceedings of The Eighteenth Text REtrieval Conference, TREC 2009, Gaithersburg, Maryland, USA, November 2009, Pages 1-6","abstract":"The Information Retrieval Lab at the University of Delaware participated in the Relevance Feedback track at TREC 2009. We used only the Category B subset of the ClueWeb collection; our preprocessing and indexing steps are described in our paper on ad hoc and diversity runs. The second year of the Relevance Feedback track focused on selection of documents for feedback. Our hypothesis is that documents that are good at distinguishing systems in terms of their effectiveness by mean average precision will also be good documents for relevance feedback. Thus we have applied the document selection algorithm MTC (Minimal Test Collections) developed by Carterette et al.  that is used in the Million Query Track for selecting documents to be judged to find the right ranking of systems. Our approach can therefore be described as \"MTC for Relevance Feedback\".","ptype":"TECHREPORT","pdf":"carterette2009b.pdf","bib":"carterette2009b.bib","meta":"carterette2009b.json"}